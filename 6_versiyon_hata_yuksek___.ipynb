{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPn8MmnE5qdr2FVpLm4rQpB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AstroBesat-SoftW/AstroBesat-SoftW/blob/main/6_versiyon_hata_yuksek___.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DOSYA ADI: teknofest_ozellestirilmis_simulasyon.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import joblib\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Concatenate, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, cohen_kappa_score\n",
        "\n",
        "# Rastgelelikleri sabitle (Tekrarlanabilirlik iÃ§in)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\">> SÄ°STEM BAÅLATILIYOR...\")\n",
        "print(\">> Hedef: Belirtilen EÄŸitim (3.940) ve Test (2.460) daÄŸÄ±lÄ±mlarÄ±na gÃ¶re hazÄ±rlÄ±k.\")\n",
        "\n",
        "# --- 1. ADIM: Ã–ZELLEÅTÄ°RÄ°LMÄ°Å VERÄ° ÃœRETÄ°MÄ° ---\n",
        "# Senin verdiÄŸin sayÄ±lara birebir uyan veri seti oluÅŸturucu\n",
        "def kategori_veri_uret(n_patojenik, n_benign, kategori_adi):\n",
        "    \"\"\"\n",
        "    Belirtilen sayÄ±da Patojenik ve Benign veri Ã¼retir.\n",
        "    \"\"\"\n",
        "    dna_seqs, prot_seqs, bio_feats, labels = [], [], [], []\n",
        "    amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
        "\n",
        "    total = n_patojenik + n_benign\n",
        "\n",
        "    for i in range(total):\n",
        "        # Etiket belirle (Ä°lk n_patojenik kadar 1, kalanÄ± 0)\n",
        "        label = 1 if i < n_patojenik else 0\n",
        "\n",
        "        # DNA/Protein (Rastgele simÃ¼lasyon)\n",
        "        dna = ''.join(np.random.choice(list('ACGT'), size=11))\n",
        "        prot = ''.join(np.random.choice(amino_acids, size=11))\n",
        "\n",
        "        # SayÄ±sal Ã–zellikler (Kategoriye gÃ¶re hafif varyasyonlar eklenebilir ama standart tutuyoruz)\n",
        "        # Patojenikse (1) risk skoru daha yÃ¼ksek Ã§Ä±kmaya meyilli olsun diye hile yapmÄ±yoruz,\n",
        "        # modelin bunu featurelardan Ã¶ÄŸrenmesini bekliyoruz. Ancak simÃ¼lasyon olduÄŸu iÃ§in\n",
        "        # etiket ile feature arasÄ±nda korelasyon kuruyoruz.\n",
        "\n",
        "        if label == 1:\n",
        "            risk = np.random.beta(5, 2) # YÃ¼ksek risk\n",
        "            cons = np.random.uniform(5, 10)\n",
        "        else:\n",
        "            risk = np.random.beta(2, 5) # DÃ¼ÅŸÃ¼k risk\n",
        "            cons = np.random.uniform(0, 5)\n",
        "\n",
        "        maf = np.random.exponential(0.05)\n",
        "        hydro = np.random.uniform(-5, 5)\n",
        "        polar = np.random.uniform(-3, 3)\n",
        "        weight = np.random.uniform(-50, 50)\n",
        "\n",
        "        dna_seqs.append(dna)\n",
        "        prot_seqs.append(prot)\n",
        "        bio_feats.append([risk, maf, cons, hydro, polar, weight])\n",
        "        labels.append(label)\n",
        "\n",
        "    return dna_seqs, prot_seqs, bio_feats, labels\n",
        "\n",
        "def veri_setini_birlestir(dagilim_sozlugu):\n",
        "    all_dna, all_prot, all_feats, all_labels = [], [], [], []\n",
        "\n",
        "    print(f\"\\n   Veri DaÄŸÄ±lÄ±mÄ± OluÅŸturuluyor:\")\n",
        "    for kat_adi, (n_pat, n_ben) in dagilim_sozlugu.items():\n",
        "        print(f\"     - {kat_adi}: {n_pat} Patojenik + {n_ben} Benign = {n_pat+n_ben}\")\n",
        "        d, p, f, l = kategori_veri_uret(n_pat, n_ben, kat_adi)\n",
        "        all_dna.extend(d)\n",
        "        all_prot.extend(p)\n",
        "        all_feats.extend(f)\n",
        "        all_labels.extend(l)\n",
        "\n",
        "    # KarÄ±ÅŸtÄ±r (Shuffle)\n",
        "    combined = list(zip(all_dna, all_prot, all_feats, all_labels))\n",
        "    np.random.shuffle(combined)\n",
        "    all_dna, all_prot, all_feats, all_labels = zip(*combined)\n",
        "\n",
        "    return list(all_dna), list(all_prot), np.array(all_feats).astype('float32'), np.array(all_labels)\n",
        "\n",
        "# 1.1. EÄÄ°TÄ°M SETÄ° OLUÅTURMA (3.940 Varyant)\n",
        "train_dagilim = {\n",
        "    \"Genel Veri Seti\": (1500, 1500),\n",
        "    \"KalÄ±tsal Kanser\": (200, 200),\n",
        "    \"FenilketonÃ¼ri (PAH)\": (200, 200),\n",
        "    \"Kistik Fibrozis (CFTR)\": (70, 70)\n",
        "}\n",
        "print(\"\\n>> [EÄÄ°TÄ°M SETÄ° HAZIRLANIYOR]\")\n",
        "dna_train, prot_train, num_train, y_train = veri_setini_birlestir(train_dagilim)\n",
        "print(f\"   >>> TOPLAM EÄÄ°TÄ°M VERÄ°SÄ°: {len(y_train)} (Beklenen: 3940)\")\n",
        "\n",
        "\n",
        "# 1.2. TEST SETÄ° OLUÅTURMA (2.460 Varyant)\n",
        "test_dagilim = {\n",
        "    \"Genel Veri Seti\": (1000, 1000),\n",
        "    \"KalÄ±tsal Kanser\": (100, 100),\n",
        "    \"FenilketonÃ¼ri (PAH)\": (100, 100),\n",
        "    \"Kistik Fibrozis (CFTR)\": (30, 30)\n",
        "}\n",
        "print(\"\\n>> [TEST SETÄ° HAZIRLANIYOR - GÄ°ZLÄ°]\")\n",
        "dna_test_ext, prot_test_ext, num_test_ext, y_test_ext = veri_setini_birlestir(test_dagilim)\n",
        "print(f\"   >>> TOPLAM TEST VERÄ°SÄ°: {len(y_test_ext)} (Beklenen: 2460)\")\n",
        "\n",
        "\n",
        "# --- 2. ADIM: TOKENIZATION VE HAZIRLIK ---\n",
        "dna_tok = Tokenizer(char_level=True)\n",
        "dna_tok.fit_on_texts(dna_train) # Sadece EÄŸitim setini gÃ¶rÃ¼r!\n",
        "\n",
        "prot_tok = Tokenizer(char_level=True)\n",
        "prot_tok.fit_on_texts(prot_train)\n",
        "\n",
        "# Sequence DÃ¶nÃ¼ÅŸÃ¼mleri\n",
        "X_dna_tr_all = pad_sequences(dna_tok.texts_to_sequences(dna_train), maxlen=11, padding='post')\n",
        "X_prot_tr_all = pad_sequences(prot_tok.texts_to_sequences(prot_train), maxlen=11, padding='post')\n",
        "\n",
        "X_dna_ext = pad_sequences(dna_tok.texts_to_sequences(dna_test_ext), maxlen=11, padding='post')\n",
        "X_prot_ext = pad_sequences(prot_tok.texts_to_sequences(prot_test_ext), maxlen=11, padding='post')\n",
        "\n",
        "# --- 3. ADIM: MODEL Ä°Ã‡Ä° BÃ–LÃœMLEME (STACKING) ---\n",
        "# Stacking iÃ§in eÄŸitim verisini Base (%75) ve Meta (%25) olarak ayÄ±rÄ±yoruz.\n",
        "X_dna_base, X_dna_meta, X_prot_base, X_prot_meta, X_num_base, X_num_meta, y_base, y_meta = train_test_split(\n",
        "    X_dna_tr_all, X_prot_tr_all, num_train, y_train,\n",
        "    test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# KlasÃ¶r kontrolÃ¼\n",
        "if not os.path.exists('model_stack'): os.makedirs('model_stack')\n",
        "\n",
        "# --- 4. ADIM: MODELLERÄ°N EÄÄ°TÄ°MÄ° ---\n",
        "\n",
        "# === [1] CNN MODELÄ° ===\n",
        "print(\"\\n>> [1/4] CNN Modeli EÄŸitiliyor...\")\n",
        "in_dna = Input(shape=(11,))\n",
        "emb_dna = Embedding(len(dna_tok.word_index)+1, 8)(in_dna)\n",
        "x1 = GlobalMaxPooling1D()(Conv1D(32, 3, activation='relu')(emb_dna))\n",
        "\n",
        "in_prot = Input(shape=(11,))\n",
        "emb_prot = Embedding(len(prot_tok.word_index)+1, 8)(in_prot)\n",
        "x2 = GlobalMaxPooling1D()(Conv1D(32, 3, activation='relu')(emb_prot))\n",
        "\n",
        "in_num_layer = Input(shape=(6,))\n",
        "x3 = BatchNormalization()(Dense(32, activation='relu')(in_num_layer))\n",
        "\n",
        "merged = Concatenate()([x1, x2, x3])\n",
        "z = Dropout(0.5)(Dense(64, activation='relu')(merged))\n",
        "out = Dense(1, activation='sigmoid')(z)\n",
        "\n",
        "model_cnn = Model(inputs=[in_dna, in_prot, in_num_layer], outputs=out)\n",
        "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Basit bir training (Callback karmaÅŸasÄ±nÄ± azalttÄ±m, odak veri yapÄ±sÄ±nda)\n",
        "model_cnn.fit(\n",
        "    [X_dna_base, X_prot_base, X_num_base], y_base,\n",
        "    validation_data=([X_dna_meta, X_prot_meta, X_num_meta], y_meta),\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    verbose=0\n",
        ")\n",
        "model_cnn.save('model_stack/best_cnn.h5')\n",
        "print(\"   >> CNN TamamlandÄ±.\")\n",
        "\n",
        "# === [2] XGBOOST MODELÄ° ===\n",
        "print(\">> [2/4] XGBoost Modeli EÄŸitiliyor...\")\n",
        "model_xgb = xgb.XGBClassifier(n_estimators=100, max_depth=4, eval_metric='logloss', use_label_encoder=False)\n",
        "model_xgb.fit(X_num_base, y_base)\n",
        "\n",
        "# === [3] LIGHTGBM MODELÄ° ===\n",
        "print(\">> [3/4] LightGBM Modeli EÄŸitiliyor...\")\n",
        "model_lgbm = lgb.LGBMClassifier(n_estimators=100, verbose=-1)\n",
        "model_lgbm.fit(X_num_base, y_base)\n",
        "\n",
        "# === [4] STACKING (META-LEARNER) ===\n",
        "print(\">> [4/4] Stacking (Meta Model) EÄŸitiliyor...\")\n",
        "\n",
        "# Meta seti tahminleri\n",
        "p_cnn = model_cnn.predict([X_dna_meta, X_prot_meta, X_num_meta], verbose=0).flatten()\n",
        "p_xgb = model_xgb.predict_proba(X_num_meta)[:, 1]\n",
        "p_lgbm = model_lgbm.predict_proba(X_num_meta)[:, 1]\n",
        "\n",
        "X_stack_meta = np.column_stack((p_cnn, p_xgb, p_lgbm))\n",
        "\n",
        "meta_model = LogisticRegression()\n",
        "meta_model.fit(X_stack_meta, y_meta)\n",
        "\n",
        "print(f\"   >> Stacking AÄŸÄ±rlÄ±klarÄ± -> CNN: {meta_model.coef_[0][0]:.2f}, XGB: {meta_model.coef_[0][1]:.2f}, LGBM: {meta_model.coef_[0][2]:.2f}\")\n",
        "\n",
        "# --- 5. ADIM: FÄ°NAL DEÄERLENDÄ°RME (SENÄ°N TEST SETÄ°NLE) ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ§ª FINAL DEÄERLENDÄ°RME (2.460 Verilik Gizli Test Seti)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "t_cnn = model_cnn.predict([X_dna_ext, X_prot_ext, num_test_ext], verbose=0).flatten()\n",
        "t_xgb = model_xgb.predict_proba(num_test_ext)[:, 1]\n",
        "t_lgbm = model_lgbm.predict_proba(num_test_ext)[:, 1]\n",
        "\n",
        "X_stack_ext = np.column_stack((t_cnn, t_xgb, t_lgbm))\n",
        "final_preds = meta_model.predict(X_stack_ext)\n",
        "\n",
        "f1 = f1_score(y_test_ext, final_preds)\n",
        "acc = accuracy_score(y_test_ext, final_preds)\n",
        "\n",
        "print(f\"| METRÄ°K               | SONUÃ‡\")\n",
        "print(f\"|----------------------|-----------\")\n",
        "print(f\"| F1 Score             | %{f1*100:.2f}\")\n",
        "print(f\"| Accuracy             | %{acc*100:.2f}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Modelleri kaydet\n",
        "joblib.dump(meta_model, 'model_stack/meta_model_final.pkl')\n",
        "joblib.dump(model_xgb, 'model_stack/xgb_final.pkl')\n",
        "joblib.dump(model_lgbm, 'model_stack/lgbm_final.pkl')\n",
        "with open('model_stack/dna_tok_final.pickle', 'wb') as f: pickle.dump(dna_tok, f)\n",
        "with open('model_stack/prot_tok_final.pickle', 'wb') as f: pickle.dump(prot_tok, f)\n",
        "\n",
        "\n",
        "# --- 6. ADIM: KULLANICI TAHMÄ°N ARAYÃœZÃœ ---\n",
        "# Senin isteÄŸin Ã¼zerine: Veri girdiÄŸinde sonucu yazan fonksiyon\n",
        "\n",
        "def tekli_tahmin_yap(dna, protein, numerical_features):\n",
        "    \"\"\"\n",
        "    KullanÄ±cÄ±dan gelen tek bir veriyi alÄ±r ve eÄŸitilen modellere sorar.\n",
        "    dna: String (11 karakter)\n",
        "    protein: String (11 karakter)\n",
        "    numerical_features: List [Risk, MAF, Cons, Hydro, Polar, Weight]\n",
        "    \"\"\"\n",
        "    print(f\"\\nâ“ YENÄ° TAHMÄ°N Ä°STEÄÄ°: DNA={dna}, PROT={protein}\")\n",
        "\n",
        "    # 1. HazÄ±rlÄ±k\n",
        "    # TokenizerlarÄ± yÃ¼kle (HafÄ±zadakileri kullanÄ±yoruz)\n",
        "    seq_dna = pad_sequences(dna_tok.texts_to_sequences([dna]), maxlen=11, padding='post')\n",
        "    seq_prot = pad_sequences(prot_tok.texts_to_sequences([protein]), maxlen=11, padding='post')\n",
        "    feats = np.array([numerical_features]).astype('float32')\n",
        "\n",
        "    # 2. Alt Model Tahminleri\n",
        "    pred_cnn = model_cnn.predict([seq_dna, seq_prot, feats], verbose=0).flatten()[0]\n",
        "    pred_xgb = model_xgb.predict_proba(feats)[:, 1][0]\n",
        "    pred_lgbm = model_lgbm.predict_proba(feats)[:, 1][0]\n",
        "\n",
        "    # 3. Stacking Tahmini\n",
        "    stack_input = np.array([[pred_cnn, pred_xgb, pred_lgbm]])\n",
        "    final_prob = meta_model.predict_proba(stack_input)[:, 1][0]\n",
        "    final_class = meta_model.predict(stack_input)[0]\n",
        "\n",
        "    # 4. SonuÃ§ YazdÄ±rma\n",
        "    durum = \"PATOJENÄ°K (HASTA)\" if final_class == 1 else \"BENIGN (Ä°YÄ° HUYLU)\"\n",
        "    renk = \"\\033[91m\" if final_class == 1 else \"\\033[92m\" # KÄ±rmÄ±zÄ± veya YeÅŸil\n",
        "    reset = \"\\033[0m\"\n",
        "\n",
        "    print(f\"   -> CNN GÃ¼veni : %{pred_cnn*100:.1f}\")\n",
        "    print(f\"   -> XGB GÃ¼veni : %{pred_xgb*100:.1f}\")\n",
        "    print(f\"   -> LGBM GÃ¼veni: %{pred_lgbm*100:.1f}\")\n",
        "    print(f\"   ----------------------------------\")\n",
        "    print(f\"   â–º SONUÃ‡: {renk}{durum}{reset} (Risk Skoru: %{final_prob*100:.2f})\")\n",
        "    return final_class\n",
        "\n",
        "# Ã–RNEK KULLANIM SENARYOSU (Bunu sen verilerini girerek test edebilirsin)\n",
        "print(\"\\n--- Ã–RNEK MANUEL TEST ---\")\n",
        "# Ã–rnek: Patojenik olma ihtimali yÃ¼ksek veriler verelim\n",
        "sample_dna = \"ACGTACGTACG\"\n",
        "sample_prot = \"MMMMMMMMMMM\"\n",
        "# Ã–zellikler: [Risk, MAF, Cons, Hydro, Polar, Weight]\n",
        "sample_feats = [0.95, 0.01, 9.5, -4.0, 2.5, 10.0]\n",
        "\n",
        "tekli_tahmin_yap(sample_dna, sample_prot, sample_feats)\n",
        "\n",
        "# Ä°kinci Ã–rnek: Benign olma ihtimali yÃ¼ksek veriler\n",
        "sample_feats_benign = [0.10, 0.40, 1.2, 1.0, 0.5, -10.0]\n",
        "tekli_tahmin_yap(\"AAAAAAAAAAA\", \"LLLLLLLLLLL\", sample_feats_benign)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTeJdlWYzOHe",
        "outputId": "0b06a1d7-84e0-4e35-c2da-078c2769665e"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> AKILLI SÄ°STEM BAÅLATILIYOR...\n",
            ">> Ayarlar: Max Epoch=50, Batch=32\n",
            "\n",
            "   Veri DaÄŸÄ±lÄ±mÄ± OluÅŸturuluyor:\n",
            "     - Genel: 1500 Patojenik + 1500 Benign\n",
            "     - Kanser: 200 Patojenik + 200 Benign\n",
            "     - PAH: 200 Patojenik + 200 Benign\n",
            "     - CFTR: 70 Patojenik + 70 Benign\n",
            "\n",
            "   Veri DaÄŸÄ±lÄ±mÄ± OluÅŸturuluyor:\n",
            "     - Genel: 1000 Patojenik + 1000 Benign\n",
            "     - Kanser: 100 Patojenik + 100 Benign\n",
            "     - PAH: 100 Patojenik + 100 Benign\n",
            "     - CFTR: 30 Patojenik + 30 Benign\n",
            "\n",
            ">> [1/4] CNN Modeli: Ä°deal Epoch HesaplanÄ±yor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   [Epoch 1/50] -> Val F1: %82.29 â­ (YENÄ° REKOR! Kaydediliyor...)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   [Epoch 2/50] -> Val F1: %90.66 â­ (YENÄ° REKOR! Kaydediliyor...)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   [Epoch 3/50] -> Val F1: %97.44 â­ (YENÄ° REKOR! Kaydediliyor...)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   [Epoch 4/50] -> Val F1: %99.20 â­ (YENÄ° REKOR! Kaydediliyor...)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   [Epoch 5/50] -> Val F1: %99.60 â­ (YENÄ° REKOR! Kaydediliyor...)\n",
            "   [Epoch 6/50] -> Val F1: %99.60 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 7/50] -> Val F1: %99.29 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 8/50] -> Val F1: %99.39 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 9/50] -> Val F1: %99.19 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 10/50] -> Val F1: %99.50 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 11/50] -> Val F1: %99.39 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 12/50] -> Val F1: %99.09 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 13/50] -> Val F1: %99.50 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 14/50] -> Val F1: %99.39 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 15/50] -> Val F1: %99.30 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 16/50] -> Val F1: %99.09 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 17/50] -> Val F1: %99.09 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 18/50] -> Val F1: %98.89 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 19/50] -> Val F1: %98.99 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 20/50] -> Val F1: %99.19 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 21/50] -> Val F1: %98.79 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 22/50] -> Val F1: %99.00 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 23/50] -> Val F1: %98.80 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 24/50] -> Val F1: %98.80 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 25/50] -> Val F1: %98.69 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 26/50] -> Val F1: %98.80 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 27/50] -> Val F1: %99.00 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 28/50] -> Val F1: %98.70 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 29/50] -> Val F1: %99.00 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 30/50] -> Val F1: %99.10 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 31/50] -> Val F1: %98.79 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 32/50] -> Val F1: %98.80 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 33/50] -> Val F1: %98.89 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 34/50] -> Val F1: %98.70 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 35/50] -> Val F1: %98.89 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 36/50] -> Val F1: %99.00 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 37/50] -> Val F1: %98.89 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 38/50] -> Val F1: %98.30 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 39/50] -> Val F1: %97.30 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 40/50] -> Val F1: %98.59 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 41/50] -> Val F1: %98.29 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 42/50] -> Val F1: %98.89 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 43/50] -> Val F1: %98.39 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 44/50] -> Val F1: %98.29 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 45/50] -> Val F1: %98.69 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 46/50] -> Val F1: %98.79 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 47/50] -> Val F1: %98.60 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 48/50] -> Val F1: %98.29 (En iyi: %99.60 @ Ep 5)\n",
            "   [Epoch 49/50] -> Val F1: %98.69 (En iyi: %99.60 @ Ep 5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   [Epoch 50/50] -> Val F1: %98.69 (En iyi: %99.60 @ Ep 5)\n",
            "\n",
            "âœ… CNN Optimizasyonu Bitti.\n",
            "   ğŸ† KAZANAN: Epoch 5 (F1: %99.60)\n",
            "   ğŸ“‚ En iyi model 'model_stack/best_cnn_optimized.h5' olarak yÃ¼klendi.\n",
            "\n",
            ">> [2/4] XGBoost EÄŸitiliyor...\n",
            ">> [3/4] LightGBM EÄŸitiliyor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [01:38:01] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> [4/4] Stacking (Meta Model) EÄŸitiliyor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ§ª FINAL DEÄERLENDÄ°RME (Gizli Test Seti)\n",
            "============================================================\n",
            "| METRÄ°K               | SONUÃ‡\n",
            "|----------------------|-----------\n",
            "| F1 Score             | %99.96\n",
            "| Accuracy             | %99.96\n",
            "| Kappa Score          | 0.9992\n",
            "============================================================\n",
            "\n",
            "--- TEST: Ã–rnek Veri ---\n",
            "\n",
            "ğŸ” Girdi: ACGTACGTACG | MMMMMMMMMMM\n",
            "   SonuÃ§: \u001b[91mPATOJENÄ°K\u001b[0m (GÃ¼ven: %99.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DOSYA ADI: canli_test_simulasyonu.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import joblib\n",
        "import time\n",
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Renk KodlarÄ± (GÃ¶rsellik iÃ§in)\n",
        "YESIL = \"\\033[92m\"\n",
        "KIRMIZI = \"\\033[91m\"\n",
        "SARI = \"\\033[93m\"\n",
        "RESET = \"\\033[0m\"\n",
        "\n",
        "print(f\"{SARI}>> SÄ°STEM BAÅLATILIYOR... MODELLER YÃœKLENÄ°YOR...{RESET}\")\n",
        "\n",
        "# --- 1. MODELLERÄ° YÃœKLE ---\n",
        "try:\n",
        "    with open('model_stack/dna_tok_final.pickle', 'rb') as f: dna_tok = pickle.load(f)\n",
        "    with open('model_stack/prot_tok_final.pickle', 'rb') as f: prot_tok = pickle.load(f)\n",
        "    model_cnn = load_model('model_stack/best_cnn.h5')\n",
        "    model_xgb = joblib.load('model_stack/xgb_final.pkl')\n",
        "    model_lgbm = joblib.load('model_stack/lgbm_final.pkl')\n",
        "    meta_model = joblib.load('model_stack/meta_model_final.pkl')\n",
        "    print(\"âœ… Modeller hafÄ±zaya alÄ±ndÄ±.\\n\")\n",
        "except:\n",
        "    print(\"âŒ HATA: Modeller bulunamadÄ±. LÃ¼tfen Ã¶nce eÄŸitim kodunu Ã§alÄ±ÅŸtÄ±rÄ±n.\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. RASTGELE VERÄ° ÃœRETÄ°CÄ° (GERÃ‡EK ETÄ°KETLÄ°) ---\n",
        "def rastgele_veri_uret():\n",
        "    \"\"\"\n",
        "    Tek bir satÄ±r rastgele veri Ã¼retir ve matematiksel kurala gÃ¶re\n",
        "    bunun GERÃ‡EKTE ne olmasÄ± gerektiÄŸini belirler.\n",
        "    \"\"\"\n",
        "    amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
        "\n",
        "    # 1. Rastgele Diziler\n",
        "    dna = ''.join(np.random.choice(list('ACGT'), size=11))\n",
        "    prot = ''.join(np.random.choice(amino_acids, size=11))\n",
        "\n",
        "    # 2. Rastgele SayÄ±sal Ã–zellikler\n",
        "    # SimÃ¼lasyon gereÄŸi: Risk ve Cons yÃ¼ksekse Patojenik olma ihtimali artar.\n",
        "    # Modelin bunu bilip bilmediÄŸini test edeceÄŸiz.\n",
        "\n",
        "    risk = np.random.beta(2, 2) # 0 ile 1 arasÄ± rastgele\n",
        "    maf = np.random.exponential(0.05)\n",
        "    cons = np.random.uniform(0, 10)\n",
        "    hydro = np.random.uniform(-5, 5)\n",
        "    polar = np.random.uniform(-3, 3)\n",
        "    weight = np.random.uniform(-50, 50)\n",
        "\n",
        "    # 3. GERÃ‡EK ETÄ°KETÄ° HESAPLA (SimÃ¼lasyon KuralÄ±)\n",
        "    # Bu formÃ¼l doÄŸanÄ±n kanunu gibi davranÄ±r. Model bunu bilmiyor, tahmin etmeye Ã§alÄ±ÅŸÄ±yor.\n",
        "    score = (risk * 0.4) + (cons/10 * 0.2) + ((0.5 - maf)*2 * 0.2) + (abs(hydro)/5 * 0.2)\n",
        "    score += np.random.normal(0, 0.05) # Biraz gÃ¼rÃ¼ltÃ¼ ekle\n",
        "\n",
        "    gercek_etiket = 1 if score > 0.65 else 0\n",
        "\n",
        "    features = [risk, maf, cons, hydro, polar, weight]\n",
        "    return dna, prot, features, gercek_etiket\n",
        "\n",
        "# --- 3. TAHMÄ°N FONKSÄ°YONU ---\n",
        "def model_tahmin_et(dna, prot, features):\n",
        "    # HazÄ±rlÄ±k\n",
        "    seq_dna = pad_sequences(dna_tok.texts_to_sequences([dna]), maxlen=11, padding='post')\n",
        "    seq_prot = pad_sequences(prot_tok.texts_to_sequences([prot]), maxlen=11, padding='post')\n",
        "    x_num = np.array([features]).astype('float32')\n",
        "\n",
        "    # Tahminler\n",
        "    p_cnn = model_cnn.predict([seq_dna, seq_prot, x_num], verbose=0).flatten()[0]\n",
        "    p_xgb = model_xgb.predict_proba(x_num)[:, 1][0]\n",
        "    p_lgbm = model_lgbm.predict_proba(x_num)[:, 1][0]\n",
        "\n",
        "    # Stacking\n",
        "    stack_input = np.array([[p_cnn, p_xgb, p_lgbm]])\n",
        "    final_pred_class = meta_model.predict(stack_input)[0]\n",
        "    final_prob = meta_model.predict_proba(stack_input)[:, 1][0]\n",
        "\n",
        "    return final_pred_class, final_prob\n",
        "\n",
        "# --- 4. CANLI TEST DÃ–NGÃœSÃœ ---\n",
        "print(f\"{'DNA':<12} {'PROTEÄ°N':<12} {'RÄ°SK':<6} | {'GERÃ‡EK':<10} | {'TAHMÄ°N':<10} | {'SONUÃ‡'}\")\n",
        "print(\"-\" * 75)\n",
        "\n",
        "dogru_sayisi = 0\n",
        "toplam_test = 20  # KaÃ§ tane test yapsÄ±n?\n",
        "\n",
        "for i in range(toplam_test):\n",
        "    # 1. Veri Ãœret\n",
        "    dna, prot, feats, gercek = rastgele_veri_uret()\n",
        "    risk_val = feats[0]\n",
        "\n",
        "    # 2. Modele Sor\n",
        "    tahmin, olasilik = model_tahmin_et(dna, prot, feats)\n",
        "\n",
        "    # 3. KarÅŸÄ±laÅŸtÄ±r\n",
        "    durum_gercek = \"HASTA\" if gercek == 1 else \"SAÄLIKLI\"\n",
        "    durum_tahmin = \"HASTA\" if tahmin == 1 else \"SAÄLIKLI\"\n",
        "\n",
        "    # SonuÃ§ Stringi HazÄ±rla\n",
        "    if gercek == tahmin:\n",
        "        sonuc_ikon = f\"{YESIL}âœ… DOÄRU{RESET}\"\n",
        "        dogru_sayisi += 1\n",
        "    else:\n",
        "        sonuc_ikon = f\"{KIRMIZI}âŒ YANLIÅ{RESET}\"\n",
        "\n",
        "    # Renkli Ã§Ä±ktÄ± iÃ§in\n",
        "    c_gercek = KIRMIZI if gercek == 1 else YESIL\n",
        "    c_tahmin = KIRMIZI if tahmin == 1 else YESIL\n",
        "\n",
        "    print(f\"{dna} {prot} {risk_val:.2f}   | {c_gercek}{durum_gercek:<10}{RESET} | {c_tahmin}{durum_tahmin:<10}{RESET} | {sonuc_ikon}\")\n",
        "\n",
        "    # KÃ¼Ã§Ã¼k bir bekleme efekti (Ä°stersen kaldÄ±rabilirsin)\n",
        "    time.sleep(0.1)\n",
        "\n",
        "# --- 5. Ã–ZET RAPOR ---\n",
        "print(\"-\" * 75)\n",
        "basari_orani = (dogru_sayisi / toplam_test) * 100\n",
        "print(f\"\\nğŸ“Š TOPLAM SONUÃ‡:\")\n",
        "print(f\"   Test Edilen Veri SayÄ±sÄ± : {toplam_test}\")\n",
        "print(f\"   DoÄŸru Bilinen           : {dogru_sayisi}\")\n",
        "print(f\"   YanlÄ±ÅŸ Bilinen          : {toplam_test - dogru_sayisi}\")\n",
        "print(f\"   MODEL BAÅARISI          : {SARI}%{basari_orani:.2f}{RESET}\")\n",
        "\n",
        "if basari_orani > 90:\n",
        "    print(f\"   YORUM: {YESIL}MÃ¼kemmel! Modelin teÅŸhis yeteneÄŸi Ã§ok yÃ¼ksek.{RESET}\")\n",
        "elif basari_orani > 75:\n",
        "    print(f\"   YORUM: {SARI}Gayet iyi. YarÄ±ÅŸma iÃ§in yeterli seviyede.{RESET}\")\n",
        "else:\n",
        "    print(f\"   YORUM: {KIRMIZI}Model biraz kararsÄ±z. EÄŸitimi tekrar etmen gerekebilir.{RESET}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XLUhhZ05MnE",
        "outputId": "f0701c2f-d9c7-47cf-ddaf-4fc2c961260f"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93m>> SÄ°STEM BAÅLATILIYOR... MODELLER YÃœKLENÄ°YOR...\u001b[0m\n",
            "âœ… Modeller hafÄ±zaya alÄ±ndÄ±.\n",
            "\n",
            "DNA          PROTEÄ°N      RÄ°SK   | GERÃ‡EK     | TAHMÄ°N     | SONUÃ‡\n",
            "---------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GACGGAATTAT RFESCYNTHVW 0.66   | \u001b[91mHASTA     \u001b[0m | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[91mâŒ YANLIÅ\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AACCCACGTAT GAGWNRMRDFA 0.50   | \u001b[91mHASTA     \u001b[0m | \u001b[91mHASTA     \u001b[0m | \u001b[92mâœ… DOÄRU\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AGGGCGTTGGC CNAAYQLAVSC 0.82   | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[92mâœ… DOÄRU\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACGGCTTGAGT FGDNQHSCHEY 0.56   | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[91mHASTA     \u001b[0m | \u001b[91mâŒ YANLIÅ\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CGGACAAGATC IAEQLSTVRNR 0.72   | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[91mHASTA     \u001b[0m | \u001b[91mâŒ YANLIÅ\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACGTCAACTCC RASWNCFVDNH 0.25   | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[91mHASTA     \u001b[0m | \u001b[91mâŒ YANLIÅ\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCGCAAAAAG RGHQFVSNTNP 0.54   | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[91mHASTA     \u001b[0m | \u001b[91mâŒ YANLIÅ\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GTCTACATCCC GRDLFQFFPLN 0.49   | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[92mâœ… DOÄRU\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TTAAGCTCGTC AECTMNWTVIM 0.14   | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[92mâœ… DOÄRU\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCAAAATTTGA AYHTQSRKMEM 0.39   | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[91mHASTA     \u001b[0m | \u001b[91mâŒ YANLIÅ\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GATCGTGTTGT VMLWADHTSIH 0.50   | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[92mâœ… DOÄRU\u001b[0m\n",
            "CCCCCTGACAC WFGDNWTHVFD 0.32   | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[92mâœ… DOÄRU\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AGCATACTGCT DQTETYYQMHN 0.86   | \u001b[91mHASTA     \u001b[0m | \u001b[91mHASTA     \u001b[0m | \u001b[92mâœ… DOÄRU\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CATTTTCAATG WKFMGGAEITD 0.85   | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[92mâœ… DOÄRU\u001b[0m\n",
            "AACTGATTGAC GSEFDDFTHQW 0.65   | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[91mHASTA     \u001b[0m | \u001b[91mâŒ YANLIÅ\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TCTAGCCACGA MMFTEQNCPKE 0.21   | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[92mâœ… DOÄRU\u001b[0m\n",
            "GAAATAGTCTT GQFPISGVYER 0.89   | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[91mHASTA     \u001b[0m | \u001b[91mâŒ YANLIÅ\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TGATTTTGCCG TLFGTCPEVYC 0.30   | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[91mHASTA     \u001b[0m | \u001b[91mâŒ YANLIÅ\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AGCCTATGCAA TFEAMSERMEE 0.45   | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[91mHASTA     \u001b[0m | \u001b[91mâŒ YANLIÅ\u001b[0m\n",
            "CACGTATAAAG SPMWTLVSIYD 0.05   | \u001b[92mSAÄLIKLI  \u001b[0m | \u001b[91mHASTA     \u001b[0m | \u001b[91mâŒ YANLIÅ\u001b[0m\n",
            "---------------------------------------------------------------------------\n",
            "\n",
            "ğŸ“Š TOPLAM SONUÃ‡:\n",
            "   Test Edilen Veri SayÄ±sÄ± : 20\n",
            "   DoÄŸru Bilinen           : 9\n",
            "   YanlÄ±ÅŸ Bilinen          : 11\n",
            "   MODEL BAÅARISI          : \u001b[93m%45.00\u001b[0m\n",
            "   YORUM: \u001b[91mModel biraz kararsÄ±z. EÄŸitimi tekrar etmen gerekebilir.\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}