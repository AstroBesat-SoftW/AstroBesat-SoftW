{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMACIL+dEniIlpHdY06Clq8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AstroBesat-SoftW/AstroBesat-SoftW/blob/main/6_versiyon_deneme_2_gelistir.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DOSYA ADI: teknofest_zorlu_egitim.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import joblib\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Concatenate, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "# --- AYARLAR ---\n",
        "MAX_EPOCH_LIMIT = 50\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Rastgelelikleri sabitle\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\">> SÄ°STEM BAÅLATILIYOR...\")\n",
        "\n",
        "# --- KULLANICIYA SORU ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ğŸ¤” EÄÄ°TÄ°M STRATEJÄ°SÄ° BELÄ°RLEME\")\n",
        "muallak_sayisi = input(\">> KaÃ§ adet 'MUALLAK' (Arada kalmÄ±ÅŸ/Zor) veri Ã¼reteyim? (Ã–rn: 500): \")\n",
        "try:\n",
        "    muallak_sayisi = int(muallak_sayisi)\n",
        "except:\n",
        "    muallak_sayisi = 0\n",
        "    print(\">> GeÃ§ersiz sayÄ± girildi, 0 kabul edildi.\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# --- 1. FONKSÄ°YONLAR ---\n",
        "\n",
        "# A) Standart Temiz Veri (Kolay/Net Ã–rnekler)\n",
        "def veri_uret_standart(n_patojenik, n_benign, kategori_adi):\n",
        "    dna_seqs, prot_seqs, bio_feats, labels = [], [], [], []\n",
        "    amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
        "    total = n_patojenik + n_benign\n",
        "\n",
        "    for i in range(total):\n",
        "        label = 1 if i < n_patojenik else 0\n",
        "        dna = ''.join(np.random.choice(list('ACGT'), size=11))\n",
        "        prot = ''.join(np.random.choice(amino_acids, size=11))\n",
        "\n",
        "        # Net ayrÄ±mlar (Hasta ise Risk yÃ¼ksek, DeÄŸilse dÃ¼ÅŸÃ¼k)\n",
        "        if label == 1:\n",
        "            risk = np.random.beta(5, 2)\n",
        "            cons = np.random.uniform(6, 10)\n",
        "        else:\n",
        "            risk = np.random.beta(2, 5)\n",
        "            cons = np.random.uniform(0, 4)\n",
        "\n",
        "        maf = np.random.exponential(0.05)\n",
        "        hydro = np.random.uniform(-5, 5)\n",
        "        polar = np.random.uniform(-3, 3)\n",
        "        weight = np.random.uniform(-50, 50)\n",
        "\n",
        "        dna_seqs.append(dna)\n",
        "        prot_seqs.append(prot)\n",
        "        bio_feats.append([risk, maf, cons, hydro, polar, weight])\n",
        "        labels.append(label)\n",
        "    return dna_seqs, prot_seqs, bio_feats, labels\n",
        "\n",
        "# B) MUALLAK VERÄ° (Modelin kafasÄ±nÄ± karÄ±ÅŸtÄ±racak zor Ã¶rnekler)\n",
        "def veri_uret_muallak(adet):\n",
        "    print(f\"   âš ï¸ {adet} adet 'Muallak/Zor' veri Ã¼retiliyor...\")\n",
        "    dna_seqs, prot_seqs, bio_feats, labels = [], [], [], []\n",
        "    amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
        "\n",
        "    for _ in range(adet):\n",
        "        # Ortada kalan Ã¶zellikler (Gri BÃ¶lge)\n",
        "        risk = np.random.uniform(0.4, 0.7) # Tam ortada riskler\n",
        "        cons = np.random.uniform(4, 7)     # Tam ortada conservation\n",
        "        maf = np.random.uniform(0.01, 0.1)\n",
        "        hydro = np.random.uniform(-2, 2)\n",
        "        polar = np.random.uniform(-1, 1)\n",
        "        weight = np.random.uniform(-10, 10)\n",
        "\n",
        "        # Etiketi formÃ¼lle belirle ama GÃœRÃœLTÃœ ekle (Bazen yanlÄ±ÅŸ etiketlensin gibi)\n",
        "        score = (risk * 0.5) + (cons/10 * 0.3) + np.random.normal(0, 0.15)\n",
        "        label = 1 if score > 0.6 else 0\n",
        "\n",
        "        dna = ''.join(np.random.choice(list('ACGT'), size=11))\n",
        "        prot = ''.join(np.random.choice(amino_acids, size=11))\n",
        "\n",
        "        dna_seqs.append(dna)\n",
        "        prot_seqs.append(prot)\n",
        "        bio_feats.append([risk, maf, cons, hydro, polar, weight])\n",
        "        labels.append(label)\n",
        "\n",
        "    return dna_seqs, prot_seqs, bio_feats, labels\n",
        "\n",
        "# --- 2. VERÄ° SETLERÄ°NÄ° BÄ°RLEÅTÄ°R ---\n",
        "def veri_hazirla():\n",
        "    all_dna, all_prot, all_feats, all_labels = [], [], [], []\n",
        "\n",
        "    # 1. Standart Veriler (Senin verdiÄŸin sayÄ±lar)\n",
        "    train_dagilim = {\"Genel\": (1500, 1500), \"Kanser\": (200, 200), \"PAH\": (200, 200), \"CFTR\": (70, 70)}\n",
        "    print(\">> Standart Veriler HazÄ±rlanÄ±yor...\")\n",
        "    for _, (p, b) in train_dagilim.items():\n",
        "        d, pr, f, l = veri_uret_standart(p, b, \"\")\n",
        "        all_dna.extend(d); all_prot.extend(pr); all_feats.extend(f); all_labels.extend(l)\n",
        "\n",
        "    # 2. Muallak Verileri Ekle\n",
        "    if muallak_sayisi > 0:\n",
        "        d_m, p_m, f_m, l_m = veri_uret_muallak(muallak_sayisi)\n",
        "        all_dna.extend(d_m); all_prot.extend(p_m); all_feats.extend(f_m); all_labels.extend(l_m)\n",
        "\n",
        "    # KarÄ±ÅŸtÄ±r\n",
        "    combined = list(zip(all_dna, all_prot, all_feats, all_labels))\n",
        "    np.random.shuffle(combined)\n",
        "    dna, prot, feats, labels = zip(*combined)\n",
        "\n",
        "    return list(dna), list(prot), np.array(feats).astype('float32'), np.array(labels)\n",
        "\n",
        "# Veriyi Ã‡ek\n",
        "dna_train, prot_train, num_train, y_train = veri_hazirla()\n",
        "print(f\">> TOPLAM EÄÄ°TÄ°M VERÄ°SÄ°: {len(y_train)} adet.\")\n",
        "\n",
        "# --- 3. TOKENIZATION ---\n",
        "dna_tok = Tokenizer(char_level=True)\n",
        "dna_tok.fit_on_texts(dna_train)\n",
        "prot_tok = Tokenizer(char_level=True)\n",
        "prot_tok.fit_on_texts(prot_train)\n",
        "\n",
        "X_dna = pad_sequences(dna_tok.texts_to_sequences(dna_train), maxlen=11, padding='post')\n",
        "X_prot = pad_sequences(prot_tok.texts_to_sequences(prot_train), maxlen=11, padding='post')\n",
        "\n",
        "# --- 4. MODEL EÄÄ°TÄ°MÄ° (KÄ±saltÄ±lmÄ±ÅŸ) ---\n",
        "X_dna_base, X_dna_meta, X_prot_base, X_prot_meta, X_num_base, X_num_meta, y_base, y_meta = train_test_split(\n",
        "    X_dna, X_prot, num_train, y_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "if not os.path.exists('model_stack'): os.makedirs('model_stack')\n",
        "\n",
        "# CNN\n",
        "print(\"\\n>> [CNN] EÄŸitiliyor (Zor verilere karÅŸÄ± direnÃ§ kazanÄ±yor)...\")\n",
        "in_dna = Input(shape=(11,)); emb_dna = Embedding(len(dna_tok.word_index)+1, 8)(in_dna)\n",
        "x1 = GlobalMaxPooling1D()(Conv1D(32, 3, activation='relu')(emb_dna))\n",
        "in_prot = Input(shape=(11,)); emb_prot = Embedding(len(prot_tok.word_index)+1, 8)(in_prot)\n",
        "x2 = GlobalMaxPooling1D()(Conv1D(32, 3, activation='relu')(emb_prot))\n",
        "in_num = Input(shape=(6,)); x3 = BatchNormalization()(Dense(32, activation='relu')(in_num))\n",
        "merged = Concatenate()([x1, x2, x3])\n",
        "out = Dense(1, activation='sigmoid')(Dropout(0.5)(Dense(64, activation='relu')(merged)))\n",
        "\n",
        "model_cnn = Model(inputs=[in_dna, in_prot, in_num], outputs=out)\n",
        "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Best Epoch Callback\n",
        "class BestEpoch(Callback):\n",
        "    def __init__(self):\n",
        "        self.best_f1 = 0\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_p = (self.model.predict([X_dna_meta, X_prot_meta, X_num_meta], verbose=0) > 0.5).astype(int)\n",
        "        f1 = f1_score(y_meta, val_p)\n",
        "        if f1 > self.best_f1:\n",
        "            self.best_f1 = f1\n",
        "            self.model.save('model_stack/best_cnn_hard.h5')\n",
        "            print(f\"   * Ep {epoch+1}: F1 %{f1*100:.2f} (Kaydedildi)\")\n",
        "\n",
        "model_cnn.fit([X_dna_base, X_prot_base, X_num_base], y_base, epochs=MAX_EPOCH_LIMIT, batch_size=32, verbose=0, callbacks=[BestEpoch()])\n",
        "model_cnn = load_model('model_stack/best_cnn_hard.h5')\n",
        "\n",
        "# XGB / LGBM\n",
        "print(\">> [XGB & LGBM] EÄŸitiliyor...\")\n",
        "model_xgb = xgb.XGBClassifier(n_estimators=100, max_depth=5).fit(X_num_base, y_base)\n",
        "model_lgbm = lgb.LGBMClassifier(n_estimators=100, verbose=-1).fit(X_num_base, y_base)\n",
        "\n",
        "# META\n",
        "print(\">> [META] Modeller BirleÅŸtiriliyor...\")\n",
        "p_cnn = model_cnn.predict([X_dna_meta, X_prot_meta, X_num_meta], verbose=0).flatten()\n",
        "p_xgb = model_xgb.predict_proba(X_num_meta)[:, 1]\n",
        "p_lgbm = model_lgbm.predict_proba(X_num_meta)[:, 1]\n",
        "meta_model = LogisticRegression().fit(np.column_stack((p_cnn, p_xgb, p_lgbm)), y_meta)\n",
        "\n",
        "# KAYDET\n",
        "joblib.dump(meta_model, 'model_stack/meta_model_final.pkl')\n",
        "joblib.dump(model_xgb, 'model_stack/xgb_final.pkl')\n",
        "joblib.dump(model_lgbm, 'model_stack/lgbm_final.pkl')\n",
        "with open('model_stack/dna_tok_final.pickle', 'wb') as f: pickle.dump(dna_tok, f)\n",
        "with open('model_stack/prot_tok_final.pickle', 'wb') as f: pickle.dump(prot_tok, f)\n",
        "\n",
        "print(f\"\\nâœ… ZORLU EÄÄ°TÄ°M TAMAMLANDI. (Eklenen Muallak Veri: {muallak_sayisi})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTeJdlWYzOHe",
        "outputId": "1b3df0e4-2f56-4aaf-aad2-20a1f36045c7"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> SÄ°STEM BAÅLATILIYOR...\n",
            "\n",
            "==================================================\n",
            "ğŸ¤” EÄÄ°TÄ°M STRATEJÄ°SÄ° BELÄ°RLEME\n",
            ">> KaÃ§ adet 'MUALLAK' (Arada kalmÄ±ÅŸ/Zor) veri Ã¼reteyim? (Ã–rn: 500): 30000\n",
            "==================================================\n",
            "\n",
            ">> Standart Veriler HazÄ±rlanÄ±yor...\n",
            "   âš ï¸ 30000 adet 'Muallak/Zor' veri Ã¼retiliyor...\n",
            ">> TOPLAM EÄÄ°TÄ°M VERÄ°SÄ°: 33940 adet.\n",
            "\n",
            ">> [CNN] EÄŸitiliyor (Zor verilere karÅŸÄ± direnÃ§ kazanÄ±yor)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   * Ep 1: F1 %40.39 (Kaydedildi)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   * Ep 2: F1 %43.51 (Kaydedildi)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   * Ep 3: F1 %43.80 (Kaydedildi)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   * Ep 4: F1 %45.19 (Kaydedildi)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   * Ep 15: F1 %45.46 (Kaydedildi)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   * Ep 20: F1 %45.48 (Kaydedildi)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   * Ep 26: F1 %45.91 (Kaydedildi)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   * Ep 41: F1 %46.01 (Kaydedildi)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   * Ep 45: F1 %46.02 (Kaydedildi)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> [XGB & LGBM] EÄŸitiliyor...\n",
            ">> [META] Modeller BirleÅŸtiriliyor...\n",
            "\n",
            "âœ… ZORLU EÄÄ°TÄ°M TAMAMLANDI. (Eklenen Muallak Veri: 30000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DOSYA ADI: canli_test_zorlu.py\n",
        "import numpy as np\n",
        "import pickle\n",
        "import joblib\n",
        "import time\n",
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Renkler\n",
        "YESIL = \"\\033[92m\"\n",
        "KIRMIZI = \"\\033[91m\"\n",
        "MAVI = \"\\033[94m\"\n",
        "RESET = \"\\033[0m\"\n",
        "\n",
        "print(\">> CANLI TEST BAÅLIYOR (Random & Formula Based)...\")\n",
        "\n",
        "# --- 1. MODELLERÄ° YÃœKLE ---\n",
        "try:\n",
        "    with open('model_stack/dna_tok_final.pickle', 'rb') as f: dna_tok = pickle.load(f)\n",
        "    with open('model_stack/prot_tok_final.pickle', 'rb') as f: prot_tok = pickle.load(f)\n",
        "    model_cnn = load_model('model_stack/best_cnn_hard.h5') # Yeni isim\n",
        "    model_xgb = joblib.load('model_stack/xgb_final.pkl')\n",
        "    model_lgbm = joblib.load('model_stack/lgbm_final.pkl')\n",
        "    meta_model = joblib.load('model_stack/meta_model_final.pkl')\n",
        "except:\n",
        "    print(\"âŒ LÃ¼tfen Ã¶nce 'teknofest_zorlu_egitim.py' dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±r!\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. TAMAMEN RANDOM VERÄ° ÃœRETÄ°CÄ° ---\n",
        "def tamamen_random_veri_uret():\n",
        "    # 1. HiÃ§bir kural gÃ¶zetmeksizin sayÄ±larÄ± salla\n",
        "    risk = np.random.uniform(0, 1)   # 0 ile 1 arasÄ± herhangi bir ÅŸey\n",
        "    cons = np.random.uniform(0, 10)  # 0 ile 10 arasÄ± herhangi bir ÅŸey\n",
        "    maf = np.random.exponential(0.05)\n",
        "    hydro = np.random.uniform(-5, 5)\n",
        "    polar = np.random.uniform(-3, 3)\n",
        "    weight = np.random.uniform(-50, 50)\n",
        "\n",
        "    # 2. DNA/Protein salla\n",
        "    dna = ''.join(np.random.choice(list('ACGT'), size=11))\n",
        "    prot = ''.join(np.random.choice(list(\"ACDEFGHIKLMNPQRSTVWY\"), size=11))\n",
        "\n",
        "    # 3. DOÄA KANUNU (Ground Truth Hesaplama)\n",
        "    # Modelin bilmediÄŸi gizli formÃ¼l bu. BakalÄ±m model bunu tahmin edebilecek mi?\n",
        "    # Skor hesapla\n",
        "    score = (risk * 0.45) + (cons/10 * 0.25) + ((0.5 - maf) * 0.1) + (abs(hydro)/5 * 0.1)\n",
        "\n",
        "    # EÄŸer skor 0.60 ile 0.70 arasÄ±ndaysa \"Muallak\" durumdur.\n",
        "    muallak_mi = 0.60 < score < 0.70\n",
        "\n",
        "    gercek_etiket = 1 if score > 0.65 else 0\n",
        "\n",
        "    return dna, prot, [risk, maf, cons, hydro, polar, weight], gercek_etiket, muallak_mi\n",
        "\n",
        "# --- 3. TAHMÄ°N MEKANÄ°ZMASI ---\n",
        "def tahmin_et(dna, prot, feats):\n",
        "    s_d = pad_sequences(dna_tok.texts_to_sequences([dna]), maxlen=11, padding='post')\n",
        "    s_p = pad_sequences(prot_tok.texts_to_sequences([prot]), maxlen=11, padding='post')\n",
        "    f_n = np.array([feats]).astype('float32')\n",
        "\n",
        "    p_cnn = model_cnn.predict([s_d, s_p, f_n], verbose=0).flatten()[0]\n",
        "    p_xgb = model_xgb.predict_proba(f_n)[:, 1][0]\n",
        "    p_lgbm = model_lgbm.predict_proba(f_n)[:, 1][0]\n",
        "\n",
        "    inp = np.array([[p_cnn, p_xgb, p_lgbm]])\n",
        "    return meta_model.predict(inp)[0], meta_model.predict_proba(inp)[:, 1][0]\n",
        "\n",
        "# --- 4. SÄ°MÃœLASYON ---\n",
        "dogru, yanlis = 0, 0\n",
        "print(f\"\\n{'DURUM':<10} | {'RÄ°SK':<5} {'CONS':<5} | {'GERÃ‡EK':<8} | {'TAHMÄ°N':<8} | {'SONUÃ‡'}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "for i in range(25): # 25 Test yapalÄ±m\n",
        "    dna, prot, feats, gercek, muallak = tamamen_random_veri_uret()\n",
        "    pred, prob = tahmin_et(dna, prot, feats)\n",
        "\n",
        "    # GÃ¶rsellik\n",
        "    str_gercek = \"HASTA\" if gercek == 1 else \"SAÄLAM\"\n",
        "    str_tahmin = \"HASTA\" if pred == 1 else \"SAÄLAM\"\n",
        "\n",
        "    if gercek == pred:\n",
        "        ikon = f\"{YESIL}âœ…{RESET}\"\n",
        "        dogru += 1\n",
        "    else:\n",
        "        ikon = f\"{KIRMIZI}âŒ{RESET}\"\n",
        "        yanlis += 1\n",
        "\n",
        "    # EÄŸer muallak (zor) veriyse yanÄ±na iÅŸaret koyalÄ±m\n",
        "    zorluk = f\"{MAVI}[ZOR]{RESET}\" if muallak else \"     \"\n",
        "\n",
        "    print(f\"{zorluk:<10} | {feats[0]:.2f}  {feats[2]:.1f}   | {str_gercek:<8} | {str_tahmin:<8} | {ikon}\")\n",
        "    time.sleep(0.1)\n",
        "\n",
        "print(\"-\" * 65)\n",
        "print(f\"ğŸ“Š SONUÃ‡: {dogru} DoÄŸru / {yanlis} YanlÄ±ÅŸ\")\n",
        "print(f\"ğŸ“ˆ BAÅARI: %{(dogru/(dogru+yanlis))*100:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XLUhhZ05MnE",
        "outputId": "edf88780-f9f2-4580-9ba8-e709507c5013"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> CANLI TEST BAÅLIYOR (Random & Formula Based)...\n",
            "\n",
            "DURUM      | RÄ°SK  CONS  | GERÃ‡EK   | TAHMÄ°N   | SONUÃ‡\n",
            "-----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           | 0.22  6.8   | SAÄLAM   | HASTA    | \u001b[91mâŒ\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           | 0.87  0.6   | SAÄLAM   | SAÄLAM   | \u001b[92mâœ…\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           | 0.57  2.5   | SAÄLAM   | SAÄLAM   | \u001b[92mâœ…\u001b[0m\n",
            "\u001b[94m[ZOR]\u001b[0m | 0.64  7.0   | SAÄLAM   | HASTA    | \u001b[91mâŒ\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94m[ZOR]\u001b[0m | 0.95  6.3   | SAÄLAM   | HASTA    | \u001b[91mâŒ\u001b[0m\n",
            "           | 0.69  6.1   | SAÄLAM   | HASTA    | \u001b[91mâŒ\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           | 0.73  2.4   | SAÄLAM   | SAÄLAM   | \u001b[92mâœ…\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94m[ZOR]\u001b[0m | 0.84  6.8   | HASTA    | HASTA    | \u001b[92mâœ…\u001b[0m\n",
            "           | 0.54  1.8   | SAÄLAM   | SAÄLAM   | \u001b[92mâœ…\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           | 0.63  5.3   | SAÄLAM   | HASTA    | \u001b[91mâŒ\u001b[0m\n",
            "           | 0.46  0.5   | SAÄLAM   | SAÄLAM   | \u001b[92mâœ…\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           | 0.57  2.1   | SAÄLAM   | SAÄLAM   | \u001b[92mâœ…\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94m[ZOR]\u001b[0m | 0.88  5.8   | SAÄLAM   | HASTA    | \u001b[91mâŒ\u001b[0m\n",
            "           | 0.78  1.5   | SAÄLAM   | SAÄLAM   | \u001b[92mâœ…\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           | 0.11  3.5   | SAÄLAM   | SAÄLAM   | \u001b[92mâœ…\u001b[0m\n",
            "           | 0.22  0.3   | SAÄLAM   | SAÄLAM   | \u001b[92mâœ…\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           | 0.67  6.2   | SAÄLAM   | HASTA    | \u001b[91mâŒ\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           | 0.80  5.9   | SAÄLAM   | HASTA    | \u001b[91mâŒ\u001b[0m\n",
            "           | 0.42  4.5   | SAÄLAM   | SAÄLAM   | \u001b[92mâœ…\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           | 0.31  1.7   | SAÄLAM   | SAÄLAM   | \u001b[92mâœ…\u001b[0m\n",
            "           | 0.44  7.2   | SAÄLAM   | HASTA    | \u001b[91mâŒ\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94m[ZOR]\u001b[0m | 0.68  6.6   | SAÄLAM   | HASTA    | \u001b[91mâŒ\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94m[ZOR]\u001b[0m | 0.98  5.3   | SAÄLAM   | HASTA    | \u001b[91mâŒ\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           | 0.58  8.0   | SAÄLAM   | HASTA    | \u001b[91mâŒ\u001b[0m\n",
            "\u001b[94m[ZOR]\u001b[0m | 0.77  9.2   | HASTA    | HASTA    | \u001b[92mâœ…\u001b[0m\n",
            "-----------------------------------------------------------------\n",
            "ğŸ“Š SONUÃ‡: 13 DoÄŸru / 12 YanlÄ±ÅŸ\n",
            "ğŸ“ˆ BAÅARI: %52.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}