{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAMMuKc+3L9Mm4w9EFT6ZT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AstroBesat-SoftW/AstroBesat-SoftW/blob/main/5_versiyon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DOSYA ADI: teknofest_gercek_simulasyon.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import joblib\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Concatenate, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, cohen_kappa_score\n",
        "\n",
        "# --- 1. ADIM: VERÄ° ÃœRETÄ°MÄ° (ÅARTNAMEYE BÄ°REBÄ°R UYGUN) ---\n",
        "print(\">> SÄ°MÃœLASYON BAÅLIYOR...\")\n",
        "print(\">> Hedef: 4000 EÄŸitim Verisi + 2500 Test Verisi (External Validation)\")\n",
        "\n",
        "def veri_uret(n, tip=\"EÄŸitim\"):\n",
        "    print(f\"   + {n} adet '{tip}' verisi Ã¼retiliyor...\")\n",
        "    dna_seqs, prot_seqs, bio_feats, labels = [], [], [], []\n",
        "    amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
        "\n",
        "    for _ in range(n):\n",
        "        # 11 harfli DNA/Protein (YarÄ±ÅŸma Ã¶rneÄŸi)\n",
        "        dna = ''.join(np.random.choice(list('ACGT'), size=11))\n",
        "        prot = ''.join(np.random.choice(amino_acids, size=11))\n",
        "\n",
        "        # SayÄ±sal Ã–zellikler\n",
        "        risk = np.random.beta(2, 2)\n",
        "        maf = np.random.exponential(0.05)\n",
        "        cons = np.random.uniform(0, 10)\n",
        "        hydro = np.random.uniform(-5, 5)\n",
        "        polar = np.random.uniform(-3, 3)\n",
        "        weight = np.random.uniform(-50, 50)\n",
        "\n",
        "        # Etiketleme (Patojenik/Benign)\n",
        "        score = (risk * 0.4) + (cons/10 * 0.2) + ((0.5 - maf)*2 * 0.2) + (abs(hydro)/5 * 0.2)\n",
        "        score += np.random.normal(0, 0.05)\n",
        "        label = 1 if score > 0.65 else 0\n",
        "\n",
        "        dna_seqs.append(dna)\n",
        "        prot_seqs.append(prot)\n",
        "        bio_feats.append([risk, maf, cons, hydro, polar, weight])\n",
        "        labels.append(label)\n",
        "\n",
        "    df_feat = pd.DataFrame(bio_feats, columns=['Risk', 'MAF', 'Cons', 'Hydro', 'Polar', 'Weight'])\n",
        "    return dna_seqs, prot_seqs, df_feat.values.astype('float32'), np.array(labels)\n",
        "\n",
        "# 1.1. EÄÄ°TÄ°M SETÄ° (Bize verilen etiketli veri) -> 4000 Adet\n",
        "dna_train, prot_train, num_train, y_train = veri_uret(4000, \"EÄÄ°TÄ°M SETÄ°\")\n",
        "\n",
        "# 1.2. TEST SETÄ° (YarÄ±ÅŸma anÄ±nda verilecek kapalÄ± veri) -> 2500 Adet\n",
        "dna_test_ext, prot_test_ext, num_test_ext, y_test_ext = veri_uret(2500, \"DIÅ TEST SETÄ°\")\n",
        "\n",
        "\n",
        "# --- 2. ADIM: VERÄ° HAZIRLIÄI ---\n",
        "# Tokenizer'Ä± sadece EÄÄ°TÄ°M setine gÃ¶re kuruyoruz (Test setini gÃ¶rmemiÅŸ gibi davranmalÄ±)\n",
        "dna_tok = Tokenizer(char_level=True)\n",
        "dna_tok.fit_on_texts(dna_train)\n",
        "\n",
        "prot_tok = Tokenizer(char_level=True)\n",
        "prot_tok.fit_on_texts(prot_train)\n",
        "\n",
        "# Sequence DÃ¶nÃ¼ÅŸÃ¼mleri\n",
        "# EÄŸitim Seti\n",
        "X_dna_tr_all = pad_sequences(dna_tok.texts_to_sequences(dna_train), maxlen=11, padding='post')\n",
        "X_prot_tr_all = pad_sequences(prot_tok.texts_to_sequences(prot_train), maxlen=11, padding='post')\n",
        "\n",
        "# External Test Seti\n",
        "X_dna_ext = pad_sequences(dna_tok.texts_to_sequences(dna_test_ext), maxlen=11, padding='post')\n",
        "X_prot_ext = pad_sequences(prot_tok.texts_to_sequences(prot_test_ext), maxlen=11, padding='post')\n",
        "\n",
        "# --- 3. ADIM: MODEL Ä°Ã‡Ä° BÃ–LÃœMLEME (STACKING STRATEJÄ°SÄ°) ---\n",
        "# 4000 verinin hepsini modele gÃ¶memeyiz, Stacking iÃ§in \"gÃ¶rÃ¼lmemiÅŸ\" veriye ihtiyaÃ§ var.\n",
        "# %75 Base Model EÄŸitimi (3000 Veri)\n",
        "# %25 Meta Model EÄŸitimi (1000 Veri) - Buna \"Validation\" da diyebiliriz.\n",
        "\n",
        "X_dna_base, X_dna_meta, X_prot_base, X_prot_meta, X_num_base, X_num_meta, y_base, y_meta = train_test_split(\n",
        "    X_dna_tr_all, X_prot_tr_all, num_train, y_train,\n",
        "    test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\n>> VERÄ° DAÄILIMI:\")\n",
        "print(f\"   - Base Model EÄŸitimi (CNN/XGB) : {len(y_base)} veri\")\n",
        "print(f\"   - Meta Model EÄŸitimi (Logistic): {len(y_meta)} veri\")\n",
        "print(f\"   - Final Test (YarÄ±ÅŸma Verisi)  : {len(y_test_ext)} veri\")\n",
        "\n",
        "\n",
        "# --- 4. ADIM: F1 TAKÄ°PÃ‡Ä°SÄ° (CALLBACK) ---\n",
        "class TeknofestBestEpoch(Callback):\n",
        "    def __init__(self, val_data, model_name):\n",
        "        super().__init__()\n",
        "        self.val_data = val_data\n",
        "        self.best_f1 = 0.0\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        # Tahmin yap\n",
        "        val_probs = self.model.predict(self.val_data[0], verbose=0)\n",
        "        val_preds = (val_probs > 0.5).astype(int)\n",
        "\n",
        "        # F1 Hesapla\n",
        "        _f1 = f1_score(self.val_data[1], val_preds)\n",
        "\n",
        "        # Ä°yileÅŸme varsa kaydet\n",
        "        if _f1 > self.best_f1:\n",
        "            print(f\"   â˜… Epoch {epoch+1}: F1 {_f1:.4f} -> {self.model_name} Kaydediliyor...\")\n",
        "            self.best_f1 = _f1\n",
        "            self.model.save(f'model_stack/{self.model_name}.h5')\n",
        "        else:\n",
        "            print(f\"     Epoch {epoch+1}: F1 {_f1:.4f} (En iyi: {self.best_f1:.4f})\")\n",
        "\n",
        "# KlasÃ¶r kontrolÃ¼\n",
        "if not os.path.exists('model_stack'): os.makedirs('model_stack')\n",
        "\n",
        "\n",
        "# --- 5. ADIM: MODELLERÄ°N EÄÄ°TÄ°MÄ° ---\n",
        "\n",
        "# === [1] CNN MODELÄ° ===\n",
        "print(\"\\n>> [1/4] CNN Modeli EÄŸitiliyor (Deep Learning)...\")\n",
        "\n",
        "in_dna = Input(shape=(11,))\n",
        "emb_dna = Embedding(len(dna_tok.word_index)+1, 8)(in_dna)\n",
        "x1 = GlobalMaxPooling1D()(Conv1D(32, 3, activation='relu')(emb_dna))\n",
        "\n",
        "in_prot = Input(shape=(11,))\n",
        "emb_prot = Embedding(len(prot_tok.word_index)+1, 8)(in_prot)\n",
        "x2 = GlobalMaxPooling1D()(Conv1D(32, 3, activation='relu')(emb_prot))\n",
        "\n",
        "in_num_layer = Input(shape=(6,))\n",
        "x3 = BatchNormalization()(Dense(32, activation='relu')(in_num_layer))\n",
        "\n",
        "merged = Concatenate()([x1, x2, x3])\n",
        "z = Dropout(0.5)(Dense(64, activation='relu')(merged))\n",
        "out = Dense(1, activation='sigmoid')(z)\n",
        "\n",
        "model_cnn = Model(inputs=[in_dna, in_prot, in_num_layer], outputs=out)\n",
        "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callback'ler\n",
        "cnn_callback = TeknofestBestEpoch(\n",
        "    val_data=([X_dna_meta, X_prot_meta, X_num_meta], y_meta),\n",
        "    model_name=\"best_cnn\"\n",
        ")\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=15, verbose=1)\n",
        "\n",
        "model_cnn.fit(\n",
        "    [X_dna_base, X_prot_base, X_num_base], y_base,\n",
        "    validation_data=([X_dna_meta, X_prot_meta, X_num_meta], y_meta),\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    verbose=0, # DetaylarÄ± kapattÄ±k, Ã¶zel callback yazacak\n",
        "    callbacks=[cnn_callback, early_stop]\n",
        ")\n",
        "\n",
        "# En iyi CNN'i geri yÃ¼kle (Stacking iÃ§in)\n",
        "print(\"   >> En iyi CNN aÄŸÄ±rlÄ±klarÄ± yÃ¼kleniyor...\")\n",
        "model_cnn = load_model('model_stack/best_cnn.h5')\n",
        "\n",
        "\n",
        "# === [2] XGBOOST MODELÄ° ===\n",
        "print(\"\\n>> [2/4] XGBoost Modeli EÄŸitiliyor...\")\n",
        "model_xgb = xgb.XGBClassifier(n_estimators=150, learning_rate=0.05, max_depth=4, eval_metric='logloss', use_label_encoder=False)\n",
        "model_xgb.fit(X_num_base, y_base)\n",
        "print(\"   >> XGBoost HazÄ±r.\")\n",
        "\n",
        "\n",
        "# === [3] LIGHTGBM MODELÄ° ===\n",
        "print(\"\\n>> [3/4] LightGBM Modeli EÄŸitiliyor...\")\n",
        "model_lgbm = lgb.LGBMClassifier(n_estimators=150, learning_rate=0.05, num_leaves=20, verbose=-1)\n",
        "model_lgbm.fit(X_num_base, y_base)\n",
        "print(\"   >> LightGBM HazÄ±r.\")\n",
        "\n",
        "\n",
        "# === [4] STACKING (META-LEARNER) ===\n",
        "print(\"\\n>> [4/4] Modellerin GÃ¼Ã§leri BirleÅŸtiriliyor (Stacking)...\")\n",
        "\n",
        "# Meta seti Ã¼zerinde tahminler (Validation verisi gibi dÃ¼ÅŸÃ¼n)\n",
        "p_cnn = model_cnn.predict([X_dna_meta, X_prot_meta, X_num_meta], verbose=0).flatten()\n",
        "p_xgb = model_xgb.predict_proba(X_num_meta)[:, 1]\n",
        "p_lgbm = model_lgbm.predict_proba(X_num_meta)[:, 1]\n",
        "\n",
        "# Bu tahminleri girdi olarak kullan\n",
        "X_stack_meta = np.column_stack((p_cnn, p_xgb, p_lgbm))\n",
        "\n",
        "# Patron (Meta) Modeli EÄŸit\n",
        "meta_model = LogisticRegression()\n",
        "meta_model.fit(X_stack_meta, y_meta)\n",
        "\n",
        "print(\"   >> Stacking Modeli HazÄ±r. AÄŸÄ±rlÄ±klar:\")\n",
        "print(f\"      CNN: {meta_model.coef_[0][0]:.2f}, XGB: {meta_model.coef_[0][1]:.2f}, LGBM: {meta_model.coef_[0][2]:.2f}\")\n",
        "\n",
        "\n",
        "# --- 6. ADIM: FÄ°NAL SINAVI (EXTERNAL VALIDATION) ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ§ª FINAL EXTERNAL VALIDATION (2500 Verilik Test)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# DÄ±ÅŸ Test Seti Ãœzerinde Tahminler\n",
        "t_cnn = model_cnn.predict([X_dna_ext, X_prot_ext, num_test_ext], verbose=0).flatten()\n",
        "t_xgb = model_xgb.predict_proba(num_test_ext)[:, 1]\n",
        "t_lgbm = model_lgbm.predict_proba(num_test_ext)[:, 1]\n",
        "\n",
        "# Stacking Tahmini\n",
        "X_stack_ext = np.column_stack((t_cnn, t_xgb, t_lgbm))\n",
        "final_preds = meta_model.predict(X_stack_ext)\n",
        "\n",
        "# Metrikler\n",
        "f1 = f1_score(y_test_ext, final_preds)\n",
        "acc = accuracy_score(y_test_ext, final_preds)\n",
        "prec = precision_score(y_test_ext, final_preds)\n",
        "rec = recall_score(y_test_ext, final_preds)\n",
        "kappa = cohen_kappa_score(y_test_ext, final_preds)\n",
        "\n",
        "print(f\"| METRÄ°K               | DEÄER     | AÃ‡IKLAMA\")\n",
        "print(f\"|----------------------|-----------|----------------------------------\")\n",
        "print(f\"| F1 Score (Ã–NEMLÄ°)    | %{f1*100:.2f}    | YarÄ±ÅŸma sÄ±ralama kriteri\")\n",
        "print(f\"| Accuracy (DoÄŸruluk)  | %{acc*100:.2f}    | Toplam doÄŸru bilme oranÄ±\")\n",
        "print(f\"| Recall (DuyarlÄ±lÄ±k)  | %{rec*100:.2f}    | HastalarÄ±n kaÃ§Ä± yakalandÄ±?\")\n",
        "print(f\"| Precision (Kesinlik) | %{prec*100:.2f}    | Hasta dediklerinin kaÃ§Ä± hasta?\")\n",
        "print(f\"| Kappa Score          | {kappa:.4f}    | { 'MÃ¼kemmel' if kappa>0.8 else 'Ä°yi' if kappa>0.6 else 'Orta'} Uyum\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Kaydet\n",
        "joblib.dump(meta_model, 'model_stack/meta_model_final.pkl')\n",
        "joblib.dump(model_xgb, 'model_stack/xgb_final.pkl')\n",
        "joblib.dump(model_lgbm, 'model_stack/lgbm_final.pkl')\n",
        "with open('model_stack/dna_tok_final.pickle', 'wb') as f: pickle.dump(dna_tok, f)\n",
        "with open('model_stack/prot_tok_final.pickle', 'wb') as f: pickle.dump(prot_tok, f)\n",
        "\n",
        "print(\"\\nâœ… SÄ°MÃœLASYON TAMAMLANDI. Dosyalar 'model_stack' klasÃ¶rÃ¼nde.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTeJdlWYzOHe",
        "outputId": "c80754ef-31d8-4d34-b72a-1118924d6904"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> SÄ°MÃœLASYON BAÅLIYOR...\n",
            ">> Hedef: 4000 EÄŸitim Verisi + 2500 Test Verisi (External Validation)\n",
            "   + 4000 adet 'EÄÄ°TÄ°M SETÄ°' verisi Ã¼retiliyor...\n",
            "   + 2500 adet 'DIÅ TEST SETÄ°' verisi Ã¼retiliyor...\n",
            "\n",
            ">> VERÄ° DAÄILIMI:\n",
            "   - Base Model EÄŸitimi (CNN/XGB) : 3000 veri\n",
            "   - Meta Model EÄŸitimi (Logistic): 1000 veri\n",
            "   - Final Test (YarÄ±ÅŸma Verisi)  : 2500 veri\n",
            "\n",
            ">> [1/4] CNN Modeli EÄŸitiliyor (Deep Learning)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   â˜… Epoch 1: F1 0.4905 -> best_cnn Kaydediliyor...\n",
            "     Epoch 2: F1 0.4635 (En iyi: 0.4905)\n",
            "     Epoch 3: F1 0.3632 (En iyi: 0.4905)\n",
            "     Epoch 4: F1 0.1479 (En iyi: 0.4905)\n",
            "     Epoch 5: F1 0.0209 (En iyi: 0.4905)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   â˜… Epoch 6: F1 0.5103 -> best_cnn Kaydediliyor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   â˜… Epoch 7: F1 0.5255 -> best_cnn Kaydediliyor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   â˜… Epoch 8: F1 0.7836 -> best_cnn Kaydediliyor...\n",
            "     Epoch 9: F1 0.7651 (En iyi: 0.7836)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   â˜… Epoch 10: F1 0.8282 -> best_cnn Kaydediliyor...\n",
            "     Epoch 11: F1 0.8140 (En iyi: 0.8282)\n",
            "     Epoch 12: F1 0.8212 (En iyi: 0.8282)\n",
            "     Epoch 13: F1 0.8094 (En iyi: 0.8282)\n",
            "     Epoch 14: F1 0.8172 (En iyi: 0.8282)\n",
            "     Epoch 15: F1 0.8111 (En iyi: 0.8282)\n",
            "     Epoch 16: F1 0.8233 (En iyi: 0.8282)\n",
            "     Epoch 17: F1 0.8176 (En iyi: 0.8282)\n",
            "     Epoch 18: F1 0.8179 (En iyi: 0.8282)\n",
            "     Epoch 19: F1 0.8185 (En iyi: 0.8282)\n",
            "     Epoch 20: F1 0.8022 (En iyi: 0.8282)\n",
            "     Epoch 21: F1 0.8171 (En iyi: 0.8282)\n",
            "     Epoch 22: F1 0.8214 (En iyi: 0.8282)\n",
            "     Epoch 23: F1 0.8255 (En iyi: 0.8282)\n",
            "     Epoch 24: F1 0.8153 (En iyi: 0.8282)\n",
            "     Epoch 25: F1 0.8134 (En iyi: 0.8282)\n",
            "     Epoch 26: F1 0.8128 (En iyi: 0.8282)\n",
            "     Epoch 27: F1 0.7955 (En iyi: 0.8282)\n",
            "     Epoch 28: F1 0.8053 (En iyi: 0.8282)\n",
            "     Epoch 29: F1 0.8197 (En iyi: 0.8282)\n",
            "Epoch 29: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   >> En iyi CNN aÄŸÄ±rlÄ±klarÄ± yÃ¼kleniyor...\n",
            "\n",
            ">> [2/4] XGBoost Modeli EÄŸitiliyor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [01:22:20] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   >> XGBoost HazÄ±r.\n",
            "\n",
            ">> [3/4] LightGBM Modeli EÄŸitiliyor...\n",
            "   >> LightGBM HazÄ±r.\n",
            "\n",
            ">> [4/4] Modellerin GÃ¼Ã§leri BirleÅŸtiriliyor (Stacking)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   >> Stacking Modeli HazÄ±r. AÄŸÄ±rlÄ±klar:\n",
            "      CNN: 3.65, XGB: 2.00, LGBM: 1.28\n",
            "\n",
            "============================================================\n",
            "ğŸ§ª FINAL EXTERNAL VALIDATION (2500 Verilik Test)\n",
            "============================================================\n",
            "| METRÄ°K               | DEÄER     | AÃ‡IKLAMA\n",
            "|----------------------|-----------|----------------------------------\n",
            "| F1 Score (Ã–NEMLÄ°)    | %82.35    | YarÄ±ÅŸma sÄ±ralama kriteri\n",
            "| Accuracy (DoÄŸruluk)  | %89.56    | Toplam doÄŸru bilme oranÄ±\n",
            "| Recall (DuyarlÄ±lÄ±k)  | %80.03    | HastalarÄ±n kaÃ§Ä± yakalandÄ±?\n",
            "| Precision (Kesinlik) | %84.82    | Hasta dediklerinin kaÃ§Ä± hasta?\n",
            "| Kappa Score          | 0.7495    | Ä°yi Uyum\n",
            "============================================================\n",
            "\n",
            "âœ… SÄ°MÃœLASYON TAMAMLANDI. Dosyalar 'model_stack' klasÃ¶rÃ¼nde.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DOSYA ADI: test_stacking_final.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, cohen_kappa_score\n",
        "\n",
        "print(\">> 1. SÄ°STEM BAÅLATILIYOR...\")\n",
        "\n",
        "# --- A. MODEL VE SÃ–ZLÃœKLERÄ° YÃœKLE (Yeni dosya isimlerine gÃ¶re) ---\n",
        "print(\">> Modeller ve SÃ¶zlÃ¼kler yÃ¼kleniyor...\")\n",
        "try:\n",
        "    # Tokenizerlar\n",
        "    with open('model_stack/dna_tok_final.pickle', 'rb') as f: dna_tok = pickle.load(f)\n",
        "    with open('model_stack/prot_tok_final.pickle', 'rb') as f: prot_tok = pickle.load(f)\n",
        "\n",
        "    # Modeller (EÄŸitim kodunda kaydettiÄŸimiz isimler)\n",
        "    model_cnn = load_model('model_stack/best_cnn.h5')\n",
        "    model_xgb = joblib.load('model_stack/xgb_final.pkl')\n",
        "    model_lgbm = joblib.load('model_stack/lgbm_final.pkl')\n",
        "    meta_model = joblib.load('model_stack/meta_model_final.pkl')\n",
        "    print(\"âœ… TÃ¼m beyinler (CNN, XGB, LGBM, Meta) baÅŸarÄ±yla yÃ¼klendi.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"âŒ HATA: Model dosyalarÄ± bulunamadÄ±. LÃ¼tfen Ã¶nce eÄŸitim kodunu Ã§alÄ±ÅŸtÄ±rÄ±n.\")\n",
        "    exit()\n",
        "\n",
        "# --- B. 2.500 ADET TEST VERÄ°SÄ° ÃœRET (EXTERNAL VALIDATION SÄ°MÃœLASYONU) ---\n",
        "print(f\"\\n>> 2. YarÄ±ÅŸma Senaryosu: 2.500 Adet YENÄ° Test Verisi Geliyor...\")\n",
        "\n",
        "def veri_uret_test(n):\n",
        "    dna_seqs, prot_seqs, bio_feats, labels = [], [], [], []\n",
        "    amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
        "\n",
        "    for _ in range(n):\n",
        "        # Rastgele DNA/Protein\n",
        "        dna = ''.join(np.random.choice(list('ACGT'), size=11))\n",
        "        prot = ''.join(np.random.choice(amino_acids, size=11))\n",
        "\n",
        "        # SayÄ±sal Ã–zellikler\n",
        "        risk = np.random.beta(2, 2)\n",
        "        maf = np.random.exponential(0.05)\n",
        "        cons = np.random.uniform(0, 10)\n",
        "        hydro = np.random.uniform(-5, 5)\n",
        "        polar = np.random.uniform(-3, 3)\n",
        "        weight = np.random.uniform(-50, 50)\n",
        "\n",
        "        # GerÃ§ek Etiket (SimÃ¼lasyon KuralÄ±)\n",
        "        score = (risk * 0.4) + (cons/10 * 0.2) + ((0.5 - maf)*2 * 0.2) + (abs(hydro)/5 * 0.2)\n",
        "        score += np.random.normal(0, 0.05)\n",
        "        label = 1 if score > 0.65 else 0\n",
        "\n",
        "        dna_seqs.append(dna)\n",
        "        prot_seqs.append(prot)\n",
        "        bio_feats.append([risk, maf, cons, hydro, polar, weight])\n",
        "        labels.append(label)\n",
        "\n",
        "    df_feat = pd.DataFrame(bio_feats, columns=['Risk', 'MAF', 'Cons', 'Hydro', 'Polar', 'Weight'])\n",
        "    return dna_seqs, prot_seqs, df_feat, np.array(labels)\n",
        "\n",
        "# Veriyi CanlÄ± Ãœret\n",
        "ham_dna, ham_prot, df_num, y_gercek = veri_uret_test(2500)\n",
        "\n",
        "# --- C. VERÄ°YÄ° MODELE HAZIRLA ---\n",
        "print(\">> Veriler iÅŸleniyor...\")\n",
        "\n",
        "# 1. Tokenize Et (Sequence)\n",
        "seq_dna = pad_sequences(dna_tok.texts_to_sequences(ham_dna), maxlen=11, padding='post')\n",
        "seq_prot = pad_sequences(prot_tok.texts_to_sequences(ham_prot), maxlen=11, padding='post')\n",
        "\n",
        "# 2. SayÄ±sal Veriyi float32 yap (Hata Ã¶nleyici)\n",
        "X_num = df_num.values.astype('float32')\n",
        "\n",
        "# --- D. TAHMÄ°N (STACKING) ---\n",
        "print(\">> Modeller tahmin yÃ¼rÃ¼tÃ¼yor...\")\n",
        "\n",
        "# 1. AÅŸama: Uzman Modellerin Tahminleri\n",
        "p_cnn = model_cnn.predict([seq_dna, seq_prot, X_num], verbose=0).flatten()\n",
        "p_xgb = model_xgb.predict_proba(X_num)[:, 1]\n",
        "p_lgbm = model_lgbm.predict_proba(X_num)[:, 1]\n",
        "\n",
        "# 2. AÅŸama: Meta Modelin KararÄ±\n",
        "stack_matrix = np.column_stack((p_cnn, p_xgb, p_lgbm))\n",
        "final_preds = meta_model.predict(stack_matrix)\n",
        "\n",
        "# --- E. RAPORLAMA ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ğŸ“Š 2.500 VERÄ°LÄ°K FÄ°NAL TEST SONUCU\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "acc = accuracy_score(y_gercek, final_preds)\n",
        "f1 = f1_score(y_gercek, final_preds)\n",
        "prec = precision_score(y_gercek, final_preds)\n",
        "rec = recall_score(y_gercek, final_preds)\n",
        "kappa = cohen_kappa_score(y_gercek, final_preds)\n",
        "\n",
        "print(f\"âœ… Accuracy  : %{acc*100:.2f}\")\n",
        "print(f\"âš–ï¸ F1 Score  : %{f1*100:.2f} (YarÄ±ÅŸma SÄ±ralama Kriteri)\")\n",
        "print(f\"ğŸ¯ Precision : %{prec*100:.2f}\")\n",
        "print(f\"ğŸ” Recall    : %{rec*100:.2f}\")\n",
        "print(f\"ğŸ² Kappa     : {kappa:.4f} \", end=\"\")\n",
        "\n",
        "if kappa > 0.8: print(\"(MÃ¼kemmel ğŸŒŸ)\")\n",
        "elif kappa > 0.6: print(\"(Ä°yi ğŸ‘)\")\n",
        "elif kappa > 0.4: print(\"(Orta ğŸ˜)\")\n",
        "else: print(\"(KÃ¶tÃ¼ âš ï¸)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# --- F. TEKÄ°L Ã–RNEK (GÃ–ZLE KONTROL) ---\n",
        "print(\"\\n>> Ã–rnek bir hasta dosyasÄ± inceleniyor...\")\n",
        "idx = np.random.randint(0, 2500) # Rastgele bir kiÅŸi seÃ§\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"Hasta ID       : {idx}\")\n",
        "print(f\"DNA Dizisi     : {ham_dna[idx]}\")\n",
        "print(f\"Protein        : {ham_prot[idx]}\")\n",
        "print(f\"Risk Skoru     : {df_num.iloc[idx]['Risk']:.4f}\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"ğŸ” CNN OlasÄ±lÄ±k : %{p_cnn[idx]*100:.1f}\")\n",
        "print(f\"ğŸŒ² XGB OlasÄ±lÄ±k : %{p_xgb[idx]*100:.1f}\")\n",
        "print(f\"ğŸƒ LGBM OlasÄ±lÄ±k: %{p_lgbm[idx]*100:.1f}\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"ğŸ¤– SÄ°STEM KARARI: {'HASTA âš ï¸' if final_preds[idx]==1 else 'SAÄLIKLI âœ…'}\")\n",
        "print(f\"âš•ï¸ GERÃ‡EK DURUM : {'HASTA âš ï¸' if y_gercek[idx]==1 else 'SAÄLIKLI âœ…'}\")\n",
        "\n",
        "if final_preds[idx] == y_gercek[idx]:\n",
        "    print(\"SONUÃ‡: âœ… DOÄRU TANI\")\n",
        "else:\n",
        "    print(\"SONUÃ‡: âŒ YANLIÅ TANI\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XLUhhZ05MnE",
        "outputId": "09bdc218-7fcf-47b4-b85f-c769682c2119"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> 1. SÄ°STEM BAÅLATILIYOR...\n",
            ">> Modeller ve SÃ¶zlÃ¼kler yÃ¼kleniyor...\n",
            "âœ… TÃ¼m beyinler (CNN, XGB, LGBM, Meta) baÅŸarÄ±yla yÃ¼klendi.\n",
            "\n",
            ">> 2. YarÄ±ÅŸma Senaryosu: 2.500 Adet YENÄ° Test Verisi Geliyor...\n",
            ">> Veriler iÅŸleniyor...\n",
            ">> Modeller tahmin yÃ¼rÃ¼tÃ¼yor...\n",
            "\n",
            "==================================================\n",
            "ğŸ“Š 2.500 VERÄ°LÄ°K FÄ°NAL TEST SONUCU\n",
            "==================================================\n",
            "âœ… Accuracy  : %88.76\n",
            "âš–ï¸ F1 Score  : %81.79 (YarÄ±ÅŸma SÄ±ralama Kriteri)\n",
            "ğŸ¯ Precision : %84.25\n",
            "ğŸ” Recall    : %79.47\n",
            "ğŸ² Kappa     : 0.7367 (Ä°yi ğŸ‘)\n",
            "==================================================\n",
            "\n",
            ">> Ã–rnek bir hasta dosyasÄ± inceleniyor...\n",
            "----------------------------------------\n",
            "Hasta ID       : 1625\n",
            "DNA Dizisi     : AATTATCCTCC\n",
            "Protein        : DHHLTPQYDGT\n",
            "Risk Skoru     : 0.7388\n",
            "----------------------------------------\n",
            "ğŸ” CNN OlasÄ±lÄ±k : %11.3\n",
            "ğŸŒ² XGB OlasÄ±lÄ±k : %15.3\n",
            "ğŸƒ LGBM OlasÄ±lÄ±k: %13.4\n",
            "----------------------------------------\n",
            "ğŸ¤– SÄ°STEM KARARI: SAÄLIKLI âœ…\n",
            "âš•ï¸ GERÃ‡EK DURUM : SAÄLIKLI âœ…\n",
            "SONUÃ‡: âœ… DOÄRU TANI\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}