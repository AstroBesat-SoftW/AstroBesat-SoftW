{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfEtdx+z0pmFoOxhHFTeMn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AstroBesat-SoftW/AstroBesat-SoftW/blob/main/Egitim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uyVT5KE5uKF7",
        "outputId": "604fb7fa-b394-4ff6-8ade-4e1f85673c88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEKNOFEST 2026 Üniversite Şartnamesine uygun 20000 veri üretiliyor...\n",
            "Patojenik (1) Sayısı: 8074\n",
            "Benign (0) Sayısı: 11926\n",
            "\n",
            "--- TEKNOFEST HİBRİT MODEL MİMARİSİ ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ Sequence_Input      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_4         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m8\u001b[0m)     │         \u001b[38;5;34m40\u001b[0m │ Sequence_Input[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ Bio_Features_Input  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │        \u001b[38;5;34m400\u001b[0m │ embedding_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │         \u001b[38;5;34m80\u001b[0m │ Bio_Features_Inp… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ conv1d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │         \u001b[38;5;34m64\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ global_max_pooli… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m1,056\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m33\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ Sequence_Input      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_4         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> │ Sequence_Input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ Bio_Features_Input  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │ embedding_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │ Bio_Features_Inp… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooli… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,673\u001b[0m (6.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,673</span> (6.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,641\u001b[0m (6.41 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,641</span> (6.41 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m32\u001b[0m (128.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> (128.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Eğitiliyor...\n",
            "Epoch 1/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6385 - loss: 0.6411 - val_accuracy: 0.6205 - val_loss: 0.6359\n",
            "Epoch 2/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8793 - loss: 0.2886 - val_accuracy: 0.8790 - val_loss: 0.2751\n",
            "Epoch 3/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8886 - loss: 0.2525 - val_accuracy: 0.8910 - val_loss: 0.2500\n",
            "Epoch 4/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8887 - loss: 0.2560 - val_accuracy: 0.9018 - val_loss: 0.2269\n",
            "Epoch 5/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8921 - loss: 0.2430 - val_accuracy: 0.8985 - val_loss: 0.2296\n",
            "Epoch 6/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8937 - loss: 0.2445 - val_accuracy: 0.8932 - val_loss: 0.2435\n",
            "Epoch 7/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8924 - loss: 0.2408 - val_accuracy: 0.9010 - val_loss: 0.2228\n",
            "Epoch 8/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8897 - loss: 0.2492 - val_accuracy: 0.9038 - val_loss: 0.2233\n",
            "Epoch 9/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8915 - loss: 0.2430 - val_accuracy: 0.8972 - val_loss: 0.2316\n",
            "Epoch 10/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8981 - loss: 0.2279 - val_accuracy: 0.8970 - val_loss: 0.2324\n",
            "\n",
            "--- TEST SONUÇLARI ---\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "TEKNOFEST Kriteri F1 Skoru: 0.8786\n",
            "\n",
            "Detaylı Rapor:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Benign       0.94      0.88      0.91      2382\n",
            "   Patojenik       0.84      0.92      0.88      1618\n",
            "\n",
            "    accuracy                           0.90      4000\n",
            "   macro avg       0.89      0.90      0.89      4000\n",
            "weighted avg       0.90      0.90      0.90      4000\n",
            "\n",
            "\n",
            "--- SENARYO ANALİZİ (YENİ VAKA TESTLERİ) ---\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "VAKA 1 (Yüksek Risk Profili): %100.00 Patojenik -> PATOJENİK ⚠️\n",
            "VAKA 2 (Temiz Profil)       : %0.10 Patojenik -> BENIGN ✅\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Concatenate, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "# --- 1. ADIM: ŞARTNAMEYE UYGUN \"SENTETİK\" VERİ ÜRETİMİ ---\n",
        "# Şartname Bölüm 3.2'deki \"Zenginleştirilmiş Varyant Profili\" simülasyonu.\n",
        "\n",
        "n_samples = 20000\n",
        "print(f\"TEKNOFEST 2026 Üniversite Şartnamesine uygun {n_samples} veri üretiliyor...\")\n",
        "\n",
        "def generate_bio_data(n):\n",
        "    sequences = []\n",
        "    bio_features = [] # Hidrofobiklik, Korunmuşluk, MAF, Risk Skoru\n",
        "    labels = []\n",
        "\n",
        "    for _ in range(n):\n",
        "        # 1. Yerel Sekans Bilgisi (Öncesi 5 + Varyant + Sonrası 5 = 11 harf) [cite: 112]\n",
        "        seq = ''.join(np.random.choice(list('ACGT'), size=11))\n",
        "\n",
        "        # 2. Biyolojik Sayısal Veriler (Rastgele üretip mantıksal ilişki kuracağız)\n",
        "\n",
        "        # In Silico Risk Skoru (0.0 - 1.0 arası): Yüksekse hastalık riski artar\n",
        "        risk_score = np.random.beta(2, 2)\n",
        "\n",
        "        # Popülasyon Sıklığı (MAF): Düşükse (Nadir) hastalık riski artar\n",
        "        maf = np.random.exponential(0.1) # Genelde düşüktür\n",
        "        if maf > 0.5: maf = 0.5\n",
        "\n",
        "        # Evrimsel Korunmuşluk (0-10): Yüksekse o bölge önemlidir, bozulursa hastalık yapar [cite: 115]\n",
        "        conservation = np.random.uniform(0, 10)\n",
        "\n",
        "        # Hidrofobiklik Değişimi (-5 ile +5 arası)\n",
        "        hydrophobicity = np.random.uniform(-5, 5)\n",
        "\n",
        "        # --- ETIKETLEME MANTIĞI (GROUND TRUTH) ---\n",
        "        # Gerçek hayatta bu kadar basit değildir ama modelin öğrenmesi için kural koyuyoruz:\n",
        "        # Eğer Risk Skoru yüksek VE Korunmuşluk yüksek VE Nadir görülüyorsa -> PATOJENİK (1)\n",
        "        # Aksi halde -> BENIGN (0)\n",
        "\n",
        "        score_calc = (risk_score * 0.5) + (conservation/10 * 0.3) + ((0.5 - maf)*2 * 0.2)\n",
        "\n",
        "        # Biraz gürültü ekleyelim ki ezberlemesin\n",
        "        score_calc += np.random.normal(0, 0.05)\n",
        "\n",
        "        if score_calc > 0.60:\n",
        "            labels.append(1) # Patojenik\n",
        "        else:\n",
        "            labels.append(0) # Benign\n",
        "\n",
        "        sequences.append(seq)\n",
        "        bio_features.append([risk_score, maf, conservation, hydrophobicity])\n",
        "\n",
        "    return sequences, np.array(bio_features), np.array(labels)\n",
        "\n",
        "sequences, X_numerical, y = generate_bio_data(n_samples)\n",
        "\n",
        "# Veri Kontrolü\n",
        "print(f\"Patojenik (1) Sayısı: {sum(y)}\")\n",
        "print(f\"Benign (0) Sayısı: {len(y) - sum(y)}\")\n",
        "\n",
        "# --- 2. ADIM: PREPROCESSING (VERİ ÖN İŞLEME) ---\n",
        "\n",
        "# A) Dizi Verisi (Sequence) İşleme\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(sequences)\n",
        "X_seq = tokenizer.texts_to_sequences(sequences)\n",
        "X_seq_pad = pad_sequences(X_seq, maxlen=11, padding='post') # 11 nükleotidlik pencere\n",
        "\n",
        "# B) Sayısal Veri (Numerical) İşleme (Normalizasyon önemlidir)\n",
        "# Basitçe 0-1 arasına sıkıştıralım (Min-Max Scaling benzeri)\n",
        "X_num = X_numerical\n",
        "# Risk skoru zaten 0-1, diğerlerini ölçeklemiyoruz şimdilik.\n",
        "\n",
        "# Eğitim/Test Ayrımı\n",
        "# Hem Sequence verisini hem Numerical veriyi ayırmalıyız\n",
        "X_seq_train, X_seq_test, X_num_train, X_num_test, y_train, y_test = train_test_split(\n",
        "    X_seq_pad, X_num, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --- 3. ADIM: HİBRİT MODEL MİMARİSİ (Sequence + Numerical) ---\n",
        "# Şartnamede \"Genetik sekans\" VE \"Biyolojik özellikler\" istendiği için\n",
        "# İki kollu (Two-Branch) bir model kuruyoruz.\n",
        "\n",
        "# KOL 1: DNA Dizisini İşleyen 1D-CNN (Sequence Branch)\n",
        "input_seq = Input(shape=(11,), name=\"Sequence_Input\")\n",
        "x1 = Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=11)(input_seq)\n",
        "x1 = Conv1D(filters=16, kernel_size=3, activation='relu')(x1)\n",
        "x1 = GlobalMaxPooling1D()(x1)\n",
        "\n",
        "# KOL 2: Sayısal Verileri İşleyen YSA (Numerical Branch) [cite: 113, 116, 117]\n",
        "input_num = Input(shape=(4,), name=\"Bio_Features_Input\")\n",
        "# 4 özellik: Risk, MAF, Conservation, Hydrophobicity\n",
        "x2 = Dense(16, activation='relu')(input_num)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "# BİRLEŞTİRME (Concatenation)\n",
        "combined = Concatenate()([x1, x2])\n",
        "\n",
        "# ORTAK KATMANLAR\n",
        "z = Dense(32, activation='relu')(combined)\n",
        "z = Dropout(0.3)(z) # Ezberlemeyi önlemek için\n",
        "output = Dense(1, activation='sigmoid')(z)\n",
        "\n",
        "# Modeli Oluştur\n",
        "model = Model(inputs=[input_seq, input_num], outputs=output)\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model Özeti\n",
        "print(\"\\n--- TEKNOFEST HİBRİT MODEL MİMARİSİ ---\")\n",
        "model.summary()\n",
        "\n",
        "# --- 4. ADIM: EĞİTİM ---\n",
        "print(\"\\nModel Eğitiliyor...\")\n",
        "history = model.fit(\n",
        "    [X_seq_train, X_num_train], y_train, # İki giriş veriyoruz\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_data=([X_seq_test, X_num_test], y_test),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --- 5. ADIM: TEST VE F1 SKORU  ---\n",
        "print(\"\\n--- TEST SONUÇLARI ---\")\n",
        "y_pred_prob = model.predict([X_seq_test, X_num_test])\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"TEKNOFEST Kriteri F1 Skoru: {f1:.4f}\")\n",
        "print(\"\\nDetaylı Rapor:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Benign', 'Patojenik']))\n",
        "\n",
        "# --- 6. ADIM: SENARYO BAZLI TEST (Özel Test Cases) ---\n",
        "# Modelin mantığı çözüp çözmediğini test edelim.\n",
        "\n",
        "print(\"\\n--- SENARYO ANALİZİ (YENİ VAKA TESTLERİ) ---\")\n",
        "\n",
        "# Vaka 1: Yüksek Risk, Yüksek Korunmuşluk, Düşük MAF -> Patojenik Olmalı\n",
        "vaka1_seq = tokenizer.texts_to_sequences([\"ACGTACGTACG\"]) # Rastgele seq\n",
        "vaka1_seq = pad_sequences(vaka1_seq, maxlen=11)\n",
        "vaka1_num = np.array([[0.95, 0.001, 9.5, 2.0]]) # Risk:0.95, MAF:0.001, Cons:9.5\n",
        "\n",
        "# Vaka 2: Düşük Risk, Yüksek MAF (Sık görülen), Düşük Korunmuşluk -> Benign Olmalı\n",
        "vaka2_seq = tokenizer.texts_to_sequences([\"TTTTTTTTTTT\"])\n",
        "vaka2_seq = pad_sequences(vaka2_seq, maxlen=11)\n",
        "vaka2_num = np.array([[0.10, 0.45, 1.0, -1.0]]) # Risk:0.10, MAF:0.45, Cons:1.0\n",
        "\n",
        "prediction1 = model.predict([vaka1_seq, vaka1_num])[0][0]\n",
        "prediction2 = model.predict([vaka2_seq, vaka2_num])[0][0]\n",
        "\n",
        "print(f\"VAKA 1 (Yüksek Risk Profili): %{prediction1*100:.2f} Patojenik -> {'PATOJENİK ⚠️' if prediction1 > 0.5 else 'HATA'}\")\n",
        "print(f\"VAKA 2 (Temiz Profil)       : %{prediction2*100:.2f} Patojenik -> {'BENIGN ✅' if prediction2 < 0.5 else 'HATA'}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Concatenate, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "# --- 1. ADIM: VERİ ÜRETİMİ ---\n",
        "n_samples = 20000\n",
        "print(f\"TEKNOFEST 2026 Üniversite Şartnamesine uygun {n_samples} veri üretiliyor...\")\n",
        "\n",
        "def generate_bio_data(n):\n",
        "    sequences = []\n",
        "    bio_features = []\n",
        "    labels = []\n",
        "\n",
        "    for _ in range(n):\n",
        "        # 1. Sekans (11 harf)\n",
        "        seq = ''.join(np.random.choice(list('ACGT'), size=11))\n",
        "\n",
        "        # 2. Biyolojik Sayısal Veriler\n",
        "        risk_score = np.random.beta(2, 2)\n",
        "        maf = np.random.exponential(0.1)\n",
        "        if maf > 0.5: maf = 0.5\n",
        "        conservation = np.random.uniform(0, 10)\n",
        "        hydrophobicity = np.random.uniform(-5, 5)\n",
        "\n",
        "        # Etiketleme Mantığı\n",
        "        score_calc = (risk_score * 0.5) + (conservation/10 * 0.3) + ((0.5 - maf)*2 * 0.2)\n",
        "        score_calc += np.random.normal(0, 0.05)\n",
        "\n",
        "        if score_calc > 0.60:\n",
        "            labels.append(1) # Patojenik\n",
        "        else:\n",
        "            labels.append(0) # Benign\n",
        "\n",
        "        sequences.append(seq)\n",
        "        bio_features.append([risk_score, maf, conservation, hydrophobicity])\n",
        "\n",
        "    return sequences, np.array(bio_features), np.array(labels)\n",
        "\n",
        "sequences, X_numerical, y = generate_bio_data(n_samples)\n",
        "\n",
        "# --- 2. ADIM: EĞİTİM VERİSİNİ TEXT OLARAK KAYDETME ---\n",
        "print(\">> Veriler 'egitim_veri_seti.txt' dosyasına kaydediliyor...\")\n",
        "\n",
        "# Verileri bir DataFrame'de toplayıp dosyaya yazıyoruz\n",
        "df_ham_veri = pd.DataFrame(X_numerical, columns=['Risk_Skoru', 'MAF_Sikligi', 'Korunmusluk', 'Hidrofobiklik'])\n",
        "df_ham_veri.insert(0, 'DNA_Dizisi', sequences) # En başa diziyi ekle\n",
        "df_ham_veri['HEDEF_ETIKET'] = y # 1: Patojenik, 0: Benign\n",
        "\n",
        "# Dosyaya yazma (Tab ile ayrılmış TXT)\n",
        "df_ham_veri.to_csv('egitim_veri_seti.txt', sep='\\t', index=False)\n",
        "print(\">> Kayıt Başarılı! (Dosya adı: egitim_veri_seti.txt)\\n\")\n",
        "\n",
        "# --- 3. ADIM: ÖN İŞLEME VE MODEL HAZIRLIĞI ---\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(sequences)\n",
        "X_seq = tokenizer.texts_to_sequences(sequences)\n",
        "X_seq_pad = pad_sequences(X_seq, maxlen=11, padding='post')\n",
        "\n",
        "X_num = X_numerical # Zaten sayısal\n",
        "\n",
        "# Eğitim ve Test Ayrımı\n",
        "X_seq_train, X_seq_test, X_num_train, X_num_test, y_train, y_test = train_test_split(\n",
        "    X_seq_pad, X_num, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Model Mimarisi (Hybrid)\n",
        "input_seq = Input(shape=(11,), name=\"Sequence_Input\")\n",
        "x1 = Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=11)(input_seq)\n",
        "x1 = Conv1D(filters=16, kernel_size=3, activation='relu')(x1)\n",
        "x1 = GlobalMaxPooling1D()(x1)\n",
        "\n",
        "input_num = Input(shape=(4,), name=\"Bio_Features_Input\")\n",
        "x2 = Dense(16, activation='relu')(input_num)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "combined = Concatenate()([x1, x2])\n",
        "z = Dense(32, activation='relu')(combined)\n",
        "z = Dropout(0.3)(z)\n",
        "output = Dense(1, activation='sigmoid')(z)\n",
        "\n",
        "model = Model(inputs=[input_seq, input_num], outputs=output)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# --- 4. ADIM: EĞİTİM ---\n",
        "print(\"Model Eğitiliyor...\")\n",
        "model.fit([X_seq_train, X_num_train], y_train, epochs=8, batch_size=64, validation_split=0.1, verbose=1)\n",
        "\n",
        "# --- 5. ADIM: TEST VE SONUÇLARI KAYDETME ---\n",
        "print(\"\\n>> Test işlemi yapılıyor ve sonuçlar kaydediliyor...\")\n",
        "\n",
        "# Tahminleri al\n",
        "y_pred_prob = model.predict([X_seq_test, X_num_test])\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# F1 Skorunu Ekrana Bas\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"Test F1 Skoru: {f1:.4f}\")\n",
        "\n",
        "# --- KAYIT İŞLEMİ (TEST SONUÇLARI) ---\n",
        "# Test dizilerini (sayısal formattan) tekrar okunabilir harflere çevirelim\n",
        "test_seqs_text = tokenizer.sequences_to_texts(X_seq_test)\n",
        "# Tokenizer araya boşluk koyar (A C G T), onları siliyoruz (ACGT) ve büyük harf yapıyoruz\n",
        "test_seqs_text = [s.replace(' ', '').upper() for s in test_seqs_text]\n",
        "\n",
        "# Sonuç Tablosu Oluşturma\n",
        "df_sonuc = pd.DataFrame(X_num_test, columns=['Risk_Skoru', 'MAF_Sikligi', 'Korunmusluk', 'Hidrofobiklik'])\n",
        "df_sonuc.insert(0, 'Test_Edilen_DNA', test_seqs_text)\n",
        "df_sonuc['Gercek_Etiket'] = y_test\n",
        "df_sonuc['Tahmin_Olasiligi'] = y_pred_prob.flatten() # Virgüllü Olasılık\n",
        "df_sonuc['Tahmin_Sinifi'] = y_pred.flatten()         # 0 veya 1\n",
        "\n",
        "# Doğru bilip bilmediğini kontrol eden sütun\n",
        "df_sonuc['SONUC_DURUMU'] = np.where(df_sonuc['Gercek_Etiket'] == df_sonuc['Tahmin_Sinifi'], 'DOGRU_BILDI', 'HATA_YAPTI')\n",
        "\n",
        "# Dosyaya kaydetme\n",
        "df_sonuc.to_csv('test_sonuc_raporu.txt', sep='\\t', index=False)\n",
        "print(\">> Test sonuçları kaydedildi! (Dosya adı: test_sonuc_raporu.txt)\")\n",
        "print(\">> Dosyayı açıp hangi veride hata yaptığını 'SONUC_DURUMU' sütunundan inceleyebilirsiniz.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTeJdlWYzOHe",
        "outputId": "1a21901d-516e-42b5-ca50-7f74cfc66b1c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEKNOFEST 2026 Üniversite Şartnamesine uygun 20000 veri üretiliyor...\n",
            ">> Veriler 'egitim_veri_seti.txt' dosyasına kaydediliyor...\n",
            ">> Kayıt Başarılı! (Dosya adı: egitim_veri_seti.txt)\n",
            "\n",
            "Model Eğitiliyor...\n",
            "Epoch 1/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6104 - loss: 0.6381 - val_accuracy: 0.6612 - val_loss: 0.5754\n",
            "Epoch 2/8\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8708 - loss: 0.3255 - val_accuracy: 0.7469 - val_loss: 0.5376\n",
            "Epoch 3/8\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8975 - loss: 0.2405 - val_accuracy: 0.8731 - val_loss: 0.2777\n",
            "Epoch 4/8\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8969 - loss: 0.2446 - val_accuracy: 0.8850 - val_loss: 0.2517\n",
            "Epoch 5/8\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8980 - loss: 0.2378 - val_accuracy: 0.8806 - val_loss: 0.2515\n",
            "Epoch 6/8\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9010 - loss: 0.2318 - val_accuracy: 0.8850 - val_loss: 0.2413\n",
            "Epoch 7/8\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8997 - loss: 0.2306 - val_accuracy: 0.8763 - val_loss: 0.2769\n",
            "Epoch 8/8\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8997 - loss: 0.2285 - val_accuracy: 0.8881 - val_loss: 0.2392\n",
            "\n",
            ">> Test işlemi yapılıyor ve sonuçlar kaydediliyor...\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Test F1 Skoru: 0.8802\n",
            ">> Test sonuçları kaydedildi! (Dosya adı: test_sonuc_raporu.txt)\n",
            ">> Dosyayı açıp hangi veride hata yaptığını 'SONUC_DURUMU' sütunundan inceleyebilirsiniz.\n"
          ]
        }
      ]
    }
  ]
}