{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmkAy0ezTTCKZceC1WlLOM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AstroBesat-SoftW/AstroBesat-SoftW/blob/main/8_version_kappa_eklendi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# --- AYARLAR ---\n",
        "DOSYA_EGITIM = \"veri.txt\"\n",
        "DOSYA_TEST = \"test.txt\"\n",
        "\n",
        "# Biyolojik Alfabe\n",
        "DNA_BASES = list(\"ACGT\")\n",
        "AMINO_ACIDS = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
        "\n",
        "# Hasta/SaÄŸlam OranÄ± AyarÄ± (0.60 = %60 Hasta Ã¼retmeye Ã§alÄ±ÅŸÄ±r)\n",
        "HEDEF_HASTA_ORANI = 0.60\n",
        "\n",
        "def rastgele_dizi(uzunluk, alfabe):\n",
        "    return \"\".join(random.choice(alfabe) for _ in range(uzunluk))\n",
        "\n",
        "def generate_pattern_sequence(length, vocab, pattern):\n",
        "    \"\"\"Motif iÃ§eren dizi oluÅŸturur (Modelin yakalamasÄ± iÃ§in ipucu)\"\"\"\n",
        "    seq = list(rastgele_dizi(length, vocab))\n",
        "    start = random.randint(0, length - len(pattern))\n",
        "    for i, char in enumerate(pattern):\n",
        "        seq[start + i] = char\n",
        "    return \"\".join(seq)\n",
        "\n",
        "def biyolojik_satir_olustur_dengeli():\n",
        "    \"\"\"\n",
        "    Bu fonksiyon rastgele deÄŸer Ã¼retmek yerine, Ã¶nce HASTA mÄ± SAÄLAM mÄ± olacaÄŸÄ±na\n",
        "    karar verir, sonra ona uygun sayÄ±sal deÄŸerler Ã¼retir.\n",
        "    BÃ¶ylece formÃ¼l yine tutarlÄ± kalÄ±r ama istediÄŸimiz kadar hasta verisi elde ederiz.\n",
        "    \"\"\"\n",
        "\n",
        "    # AdÄ±m 1: Bu satÄ±rÄ±n ne olmasÄ±nÄ± istiyoruz? (Hedef Belirleme)\n",
        "    if random.random() < HEDEF_HASTA_ORANI:\n",
        "        niyet = \"HASTA\"\n",
        "    else:\n",
        "        niyet = \"SAGLAM\"\n",
        "\n",
        "    # AdÄ±m 2: Niyete uygun aralÄ±klardan sayÄ± seÃ§ (Bias Injection)\n",
        "    if niyet == \"HASTA\":\n",
        "        # Hasta olmasÄ± iÃ§in yÃ¼ksek risk faktÃ¶rleri seÃ§iyoruz\n",
        "        # FormÃ¼l > 0.60 Ã§Ä±kmalÄ±\n",
        "        risk = random.uniform(0.55, 1.00)    # YÃ¼ksek Risk\n",
        "        maf = random.uniform(0.00, 0.15)     # DÃ¼ÅŸÃ¼k MAF (Nadir)\n",
        "        cons = random.uniform(6.0, 10.0)     # YÃ¼ksek KorunmuÅŸluk\n",
        "        hydro = random.uniform(-5.0, -1.0)   # Negatif\n",
        "        polar = random.uniform(2.0, 5.0)     # Pozitif\n",
        "        weight_base = random.uniform(50.0, 90.0)\n",
        "    else:\n",
        "        # SaÄŸlam olmasÄ± iÃ§in dÃ¼ÅŸÃ¼k risk faktÃ¶rleri\n",
        "        risk = random.uniform(0.00, 0.50)    # DÃ¼ÅŸÃ¼k Risk\n",
        "        maf = random.uniform(0.20, 0.50)     # YÃ¼ksek MAF (YaygÄ±n)\n",
        "        cons = random.uniform(0.0, 5.0)      # DÃ¼ÅŸÃ¼k KorunmuÅŸluk\n",
        "        hydro = random.uniform(-1.0, 5.0)\n",
        "        polar = random.uniform(-5.0, 1.0)\n",
        "        weight_base = random.uniform(10.0, 50.0)\n",
        "\n",
        "    # AdÄ±m 3: FormÃ¼lÃ¼ Uygula (Matematiksel TutarlÄ±lÄ±k Ä°Ã§in)\n",
        "    # DeÄŸerleri biz seÃ§tik ama etiket yine formÃ¼lden Ã§Ä±kmalÄ± ki model matematiÄŸi Ã¶ÄŸrensin.\n",
        "    score = (risk * 0.50) + (cons/10 * 0.30) + ((0.5 - maf) * 0.1) + (abs(hydro)/5 * 0.1)\n",
        "\n",
        "    # Hafif gÃ¼rÃ¼ltÃ¼ ekle\n",
        "    noise = random.uniform(-0.02, 0.02)\n",
        "    final_score = score + noise\n",
        "\n",
        "    # AdÄ±m 4: Etiketleme ve Dizi OluÅŸturma\n",
        "    if final_score > 0.60:\n",
        "        etiket = 1 # HASTA\n",
        "        # Hasta dizilerinde motif (pattern) olma ihtimali yÃ¼ksek\n",
        "        dna = generate_pattern_sequence(11, DNA_BASES, \"CGT\") if random.random() > 0.15 else rastgele_dizi(11, DNA_BASES)\n",
        "        prot = generate_pattern_sequence(11, AMINO_ACIDS, \"WW\") if random.random() > 0.15 else rastgele_dizi(11, AMINO_ACIDS)\n",
        "        weight = weight_base + 10\n",
        "    else:\n",
        "        etiket = 0 # SAÄLAM\n",
        "        # SaÄŸlamlarda motif Ã§ok nadir olsun\n",
        "        dna = generate_pattern_sequence(11, DNA_BASES, \"AAA\") if random.random() > 0.9 else rastgele_dizi(11, DNA_BASES)\n",
        "        prot = rastgele_dizi(11, AMINO_ACIDS)\n",
        "        weight = weight_base - 10\n",
        "\n",
        "    return [\n",
        "        dna, prot,\n",
        "        f\"{risk:.4f}\", f\"{maf:.4f}\", f\"{cons:.4f}\",\n",
        "        f\"{hydro:.4f}\", f\"{polar:.4f}\", f\"{weight:.4f}\",\n",
        "        str(etiket)\n",
        "    ]\n",
        "\n",
        "def dosya_uret_ve_kaydet(dosya_adi, toplam_adet):\n",
        "    print(f\"\\nâš™ï¸ {dosya_adi} hazÄ±rlanÄ±yor... (Hedef: {toplam_adet} satÄ±r)\")\n",
        "\n",
        "    veri_listesi = []\n",
        "    hasta_sayaci = 0\n",
        "\n",
        "    for _ in range(toplam_adet):\n",
        "        satir = biyolojik_satir_olustur_dengeli()\n",
        "        if satir[-1] == \"1\": hasta_sayaci += 1\n",
        "        veri_listesi.append(satir)\n",
        "\n",
        "    # KarÄ±ÅŸtÄ±r (Model sÄ±rayÄ± ezberlemesin)\n",
        "    random.shuffle(veri_listesi)\n",
        "\n",
        "    with open(dosya_adi, \"w\", encoding=\"utf-8\") as f:\n",
        "        for satir in veri_listesi:\n",
        "            f.write(\",\".join(satir) + \"\\n\")\n",
        "\n",
        "    oran = (hasta_sayaci / toplam_adet) * 100\n",
        "    print(f\"âœ… {dosya_adi} bitti.\")\n",
        "    print(f\"   ğŸ“Š Ä°statistik: {hasta_sayaci} HASTA (%{oran:.1f}) | {toplam_adet - hasta_sayaci} SAÄLAM\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*60)\n",
        "    print(\"ğŸ§¬ GÃœNCELLENMÄ°Å HASTA ODAKLI VERÄ° ÃœRETÄ°CÄ°\")\n",
        "    print(\"   (Strateji: Verilerin ~%60'Ä± HASTA olacak ÅŸekilde ayarlandÄ±)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    try:\n",
        "        n_train = int(input(\">> EÄŸitim iÃ§in kaÃ§ veri Ã¼retilsin? (Ã–rn: 5000): \"))\n",
        "        n_test = int(input(\">> Test iÃ§in kaÃ§ veri Ã¼retilsin? (Ã–rn: 1000): \"))\n",
        "    except:\n",
        "        print(\"âš ï¸ SayÄ± girilmedi, varsayÄ±lanlar kullanÄ±lÄ±yor (5000/1000).\")\n",
        "        n_train, n_test = 5000, 1000\n",
        "\n",
        "    dosya_uret_ve_kaydet(DOSYA_EGITIM, n_train)\n",
        "    dosya_uret_ve_kaydet(DOSYA_TEST, n_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WNtVohosK5p",
        "outputId": "57a6feaa-eb5a-429f-d6ef-0ce0ddb6cb0c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ğŸ§¬ GÃœNCELLENMÄ°Å HASTA ODAKLI VERÄ° ÃœRETÄ°CÄ°\n",
            "   (Strateji: Verilerin ~%60'Ä± HASTA olacak ÅŸekilde ayarlandÄ±)\n",
            "============================================================\n",
            ">> EÄŸitim iÃ§in kaÃ§ veri Ã¼retilsin? (Ã–rn: 5000): 100000\n",
            ">> Test iÃ§in kaÃ§ veri Ã¼retilsin? (Ã–rn: 1000): 10000\n",
            "\n",
            "âš™ï¸ veri.txt hazÄ±rlanÄ±yor... (Hedef: 100000 satÄ±r)\n",
            "âœ… veri.txt bitti.\n",
            "   ğŸ“Š Ä°statistik: 57211 HASTA (%57.2) | 42789 SAÄLAM\n",
            "\n",
            "âš™ï¸ test.txt hazÄ±rlanÄ±yor... (Hedef: 10000 satÄ±r)\n",
            "âœ… test.txt bitti.\n",
            "   ğŸ“Š Ä°statistik: 5710 HASTA (%57.1) | 4290 SAÄLAM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import joblib\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "# Keras / TensorFlow\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Concatenate, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "# Scikit-Learn Modelleri\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "\n",
        "# --- AYARLAR ---\n",
        "MAX_EPOCH_LIMIT = 40\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Rastgelelikleri sabitle\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\">> SÄ°STEM BAÅLATILIYOR (GÃœNCELLENMÄ°Å EÄÄ°TÄ°M MODU)...\")\n",
        "\n",
        "# --- 1. DOSYA OKUMA VE HAZIRLAMA FONKSÄ°YONLARI ---\n",
        "\n",
        "def dosya_oku(dosya_yolu):\n",
        "    \"\"\"\n",
        "    Belirtilen txt dosyasÄ±nÄ± okur.\n",
        "    Beklenen Format: DNA,PROTEIN,Risk,Maf,Cons,Hydro,Polar,Weight,Label\n",
        "    \"\"\"\n",
        "    if not os.path.exists(dosya_yolu):\n",
        "        print(f\"âŒ HATA: '{dosya_yolu}' bulunamadÄ±! LÃ¼tfen dosyayÄ± oluÅŸturun.\")\n",
        "        return [], [], [], []\n",
        "\n",
        "    print(f\">> '{dosya_yolu}' dosyasÄ± okunuyor...\")\n",
        "\n",
        "    dna_seqs, prot_seqs, bio_feats, labels = [], [], [], []\n",
        "\n",
        "    with open(dosya_yolu, \"r\", encoding=\"utf-8\") as f:\n",
        "        satirlar = f.readlines()\n",
        "\n",
        "    for i, satir in enumerate(satirlar):\n",
        "        satir = satir.strip()\n",
        "        if not satir: continue # BoÅŸ satÄ±rÄ± atla\n",
        "\n",
        "        # VirgÃ¼lle ayÄ±r\n",
        "        cols = satir.split(',')\n",
        "\n",
        "        # Basit hata kontrolÃ¼ (9 sÃ¼tun olmalÄ±: 2 dizi + 6 Ã¶zellik + 1 etiket)\n",
        "        if len(cols) != 9:\n",
        "            print(f\"âš ï¸ UyarÄ±: SatÄ±r {i+1} formatÄ± bozuk, atlandÄ±. SÃ¼tun sayÄ±sÄ±: {len(cols)}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            d = cols[0].strip().upper()\n",
        "            p = cols[1].strip().upper()\n",
        "            # SayÄ±sal Ã¶zellikleri Ã§ek\n",
        "            feats = [float(x) for x in cols[2:8]]\n",
        "            l = int(cols[8])\n",
        "\n",
        "            dna_seqs.append(d)\n",
        "            prot_seqs.append(p)\n",
        "            bio_feats.append(feats)\n",
        "            labels.append(l)\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Hata (SatÄ±r {i+1}): {e}\")\n",
        "\n",
        "    return dna_seqs, prot_seqs, np.array(bio_feats).astype('float32'), np.array(labels)\n",
        "\n",
        "# --- 2. VERÄ°LERÄ° YÃœKLE ---\n",
        "\n",
        "# EÄŸitim verisini oku (veri.txt)\n",
        "print(\"\\n--- EÄÄ°TÄ°M VERÄ°SÄ° YÃœKLENÄ°YOR ---\")\n",
        "dna_train_raw, prot_train_raw, num_train_raw, y_train_raw = dosya_oku(\"veri.txt\")\n",
        "\n",
        "if len(y_train_raw) == 0:\n",
        "    print(\"âŒ EÄŸitim verisi boÅŸ! Program durduruluyor.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"âœ… 'veri.txt' baÅŸarÄ±yla yÃ¼klendi. Toplam kayÄ±t: {len(y_train_raw)}\")\n",
        "\n",
        "# Test verisini oku (test.txt)\n",
        "print(\"\\n--- TEST VERÄ°SÄ° YÃœKLENÄ°YOR ---\")\n",
        "dna_test_raw, prot_test_raw, num_test_raw, y_test_raw = dosya_oku(\"test.txt\")\n",
        "print(f\"âœ… 'test.txt' baÅŸarÄ±yla yÃ¼klendi. Toplam kayÄ±t: {len(y_test_raw)}\")\n",
        "\n",
        "# --- 3. TOKENIZATION (Sadece EÄŸitim Verisine GÃ¶re EÄŸitilir) ---\n",
        "print(\"\\n>> Tokenizer eÄŸitiliyor...\")\n",
        "dna_tok = Tokenizer(char_level=True)\n",
        "dna_tok.fit_on_texts(dna_train_raw) # Sadece eÄŸitim setini gÃ¶rsÃ¼n\n",
        "\n",
        "prot_tok = Tokenizer(char_level=True)\n",
        "prot_tok.fit_on_texts(prot_train_raw)\n",
        "\n",
        "# Hem Train hem Test verisini sayÄ±sal diziye Ã§evir\n",
        "# Train\n",
        "X_dna_train_all = pad_sequences(dna_tok.texts_to_sequences(dna_train_raw), maxlen=11, padding='post')\n",
        "X_prot_train_all = pad_sequences(prot_tok.texts_to_sequences(prot_train_raw), maxlen=11, padding='post')\n",
        "X_num_train_all = num_train_raw\n",
        "y_train_all = y_train_raw\n",
        "\n",
        "# Test (EÄŸer test verisi varsa dÃ¶nÃ¼ÅŸtÃ¼r, yoksa boÅŸ geÃ§)\n",
        "if len(y_test_raw) > 0:\n",
        "    X_dna_test = pad_sequences(dna_tok.texts_to_sequences(dna_test_raw), maxlen=11, padding='post')\n",
        "    X_prot_test = pad_sequences(prot_tok.texts_to_sequences(prot_test_raw), maxlen=11, padding='post')\n",
        "    X_num_test = num_test_raw\n",
        "    y_test = y_test_raw\n",
        "else:\n",
        "    print(\"âš ï¸ UYARI: test.txt boÅŸ veya bulunamadÄ±. Final test yapÄ±lamayacak.\")\n",
        "    X_dna_test, X_prot_test, X_num_test, y_test = None, None, None, None\n",
        "\n",
        "\n",
        "# --- 4. DATA SPLIT (STACKING Ä°Ã‡Ä°N) ---\n",
        "# Stacking yapabilmek iÃ§in \"veri.txt\" iÃ§indeki veriyi Base Modeller ve Meta Model iÃ§in ikiye bÃ¶lÃ¼yoruz.\n",
        "print(\"\\n>> EÄŸitim verisi 'Base' ve 'Meta' eÄŸitim setlerine ayrÄ±lÄ±yor...\")\n",
        "X_dna_base, X_dna_meta, X_prot_base, X_prot_meta, X_num_base, X_num_meta, y_base, y_meta = train_test_split(\n",
        "    X_dna_train_all, X_prot_train_all, X_num_train_all, y_train_all,\n",
        "    test_size=0.30, random_state=42 # %70 Base Model EÄŸitimi, %30 Meta Model EÄŸitimi\n",
        ")\n",
        "\n",
        "if not os.path.exists('model_stack'): os.makedirs('model_stack')\n",
        "\n",
        "# --- 5. MODEL EÄÄ°TÄ°MÄ° (STACKING BASE MODELS - GÃœNCELLENMÄ°Å KISIM) ---\n",
        "\n",
        "# --- MODEL 1: CNN (Derin Ã–ÄŸrenme) ---\n",
        "print(\"\\n>> [1/9] CNN (Deep Learning) EÄŸitiliyor...\")\n",
        "in_dna = Input(shape=(11,)); emb_dna = Embedding(len(dna_tok.word_index)+1, 12)(in_dna)\n",
        "x1 = GlobalMaxPooling1D()(Conv1D(64, 3, activation='relu')(emb_dna))\n",
        "in_prot = Input(shape=(11,)); emb_prot = Embedding(len(prot_tok.word_index)+1, 12)(in_prot)\n",
        "x2 = GlobalMaxPooling1D()(Conv1D(64, 3, activation='relu')(emb_prot))\n",
        "in_num = Input(shape=(6,)); x3 = BatchNormalization()(Dense(32, activation='relu')(in_num))\n",
        "merged = Concatenate()([x1, x2, x3])\n",
        "out = Dense(1, activation='sigmoid')(Dropout(0.3)(Dense(32, activation='relu')(merged)))\n",
        "\n",
        "model_cnn = Model(inputs=[in_dna, in_prot, in_num], outputs=out)\n",
        "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "class BestEpoch(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch % 10 == 0: print(f\"    - Epoch {epoch} bitti. Loss: {logs['loss']:.4f}\")\n",
        "\n",
        "# CNN iÃ§in class weight hesaplama (Manuel)\n",
        "cw = {0: 1.0, 1: (np.sum(y_base==0) / np.sum(y_base==1))} if np.sum(y_base==1) > 0 else {0:1.0, 1:1.0}\n",
        "model_cnn.fit([X_dna_base, X_prot_base, X_num_base], y_base, epochs=30, batch_size=32, verbose=0, callbacks=[BestEpoch()], class_weight=cw)\n",
        "model_cnn.save('model_stack/model_cnn.h5')\n",
        "\n",
        "# --- DÄ°ÄER MODELLER (Class Weight Balanced Eklendi) ---\n",
        "print(\">> [2/9] XGBoost EÄŸitiliyor... (Scale Pos Weight Aktif)\")\n",
        "# XGBoost iÃ§in scale_pos_weight hesaplama\n",
        "neg_count = np.sum(y_base == 0)\n",
        "pos_count = np.sum(y_base == 1)\n",
        "spw = neg_count / pos_count if pos_count > 0 else 1.0\n",
        "model_xgb = xgb.XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.05, scale_pos_weight=spw).fit(X_num_base, y_base)\n",
        "\n",
        "print(\">> [3/9] LightGBM EÄŸitiliyor... (Balanced)\")\n",
        "model_lgbm = lgb.LGBMClassifier(n_estimators=200, class_weight='balanced', verbose=-1).fit(X_num_base, y_base)\n",
        "\n",
        "print(\">> [4/9] Random Forest EÄŸitiliyor... (Balanced)\")\n",
        "model_rf = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42).fit(X_num_base, y_base)\n",
        "\n",
        "print(\">> [5/9] Decision Tree EÄŸitiliyor... (Balanced)\")\n",
        "model_dt = DecisionTreeClassifier(class_weight='balanced', random_state=42).fit(X_num_base, y_base)\n",
        "\n",
        "print(\">> [6/9] SVM EÄŸitiliyor... (Balanced)\")\n",
        "model_svm = SVC(probability=True, class_weight='balanced', random_state=42).fit(X_num_base, y_base)\n",
        "\n",
        "print(\">> [7/9] KNN EÄŸitiliyor... (Distance Weighting)\")\n",
        "model_knn = KNeighborsClassifier(n_neighbors=5, weights='distance').fit(X_num_base, y_base)\n",
        "\n",
        "print(\">> [8/9] Naive Bayes EÄŸitiliyor...\")\n",
        "model_nb = GaussianNB().fit(X_num_base, y_base)\n",
        "\n",
        "print(\">> [9/9] Lojistik Regresyon (Base) EÄŸitiliyor... (Balanced)\")\n",
        "model_lr_base = LogisticRegression(max_iter=2000, class_weight='balanced').fit(X_num_base, y_base)\n",
        "\n",
        "# --- 6. META MODEL (STACKING) EÄÄ°TÄ°MÄ° ---\n",
        "print(\"\\n>> [META] Tahminler ToplanÄ±yor (Stacking)...\")\n",
        "\n",
        "# Meta seti Ã¼zerinde tahminler al\n",
        "p_cnn = model_cnn.predict([X_dna_meta, X_prot_meta, X_num_meta], verbose=0).flatten()\n",
        "p_xgb = model_xgb.predict_proba(X_num_meta)[:, 1]\n",
        "p_lgbm = model_lgbm.predict_proba(X_num_meta)[:, 1]\n",
        "p_rf = model_rf.predict_proba(X_num_meta)[:, 1]\n",
        "p_dt = model_dt.predict_proba(X_num_meta)[:, 1]\n",
        "p_svm = model_svm.predict_proba(X_num_meta)[:, 1]\n",
        "p_knn = model_knn.predict_proba(X_num_meta)[:, 1]\n",
        "p_nb = model_nb.predict_proba(X_num_meta)[:, 1]\n",
        "p_lr = model_lr_base.predict_proba(X_num_meta)[:, 1]\n",
        "\n",
        "# Stack matrisini oluÅŸtur\n",
        "stack_X_meta = np.column_stack((p_cnn, p_xgb, p_lgbm, p_rf, p_dt, p_svm, p_knn, p_nb, p_lr))\n",
        "\n",
        "# Meta Modeli EÄŸit\n",
        "print(\">> Meta Model (Hakem) eÄŸitiliyor...\")\n",
        "meta_model = LogisticRegression(class_weight='balanced') # Meta modele de balanced dedik\n",
        "meta_model.fit(stack_X_meta, y_meta)\n",
        "\n",
        "# --- 7. FÄ°NAL TEST (test.txt Ä°LE) ---\n",
        "if X_dna_test is not None:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"ğŸ† FÄ°NAL TEST SONUÃ‡LARI (Test.txt Verisi Ãœzerinde)\")\n",
        "\n",
        "    # 1. Base modeller test verisini tahmin eder\n",
        "    t_cnn = model_cnn.predict([X_dna_test, X_prot_test, X_num_test], verbose=0).flatten()\n",
        "    t_xgb = model_xgb.predict_proba(X_num_test)[:, 1]\n",
        "    t_lgbm = model_lgbm.predict_proba(X_num_test)[:, 1]\n",
        "    t_rf = model_rf.predict_proba(X_num_test)[:, 1]\n",
        "    t_dt = model_dt.predict_proba(X_num_test)[:, 1]\n",
        "    t_svm = model_svm.predict_proba(X_num_test)[:, 1]\n",
        "    t_knn = model_knn.predict_proba(X_num_test)[:, 1]\n",
        "    t_nb = model_nb.predict_proba(X_num_test)[:, 1]\n",
        "    t_lr = model_lr_base.predict_proba(X_num_test)[:, 1]\n",
        "\n",
        "    # 2. Stack matrisi oluÅŸturulur\n",
        "    stack_X_final = np.column_stack((t_cnn, t_xgb, t_lgbm, t_rf, t_dt, t_svm, t_knn, t_nb, t_lr))\n",
        "\n",
        "    # 3. Meta model son kararÄ± verir\n",
        "    final_preds = meta_model.predict(stack_X_final)\n",
        "\n",
        "    # 4. Metrikler\n",
        "    final_acc = accuracy_score(y_test, final_preds)\n",
        "    final_f1 = f1_score(y_test, final_preds)\n",
        "\n",
        "    print(f\"âœ… Accuracy : %{final_acc*100:.2f}\")\n",
        "    print(f\"ğŸš€ F1 Score : %{final_f1*100:.2f}\")\n",
        "    print(\"-\" * 50)\n",
        "    print(\"DetaylÄ± Rapor:\")\n",
        "    print(classification_report(y_test, final_preds, target_names=[\"Negatif\", \"Pozitif\"]))\n",
        "    print(\"=\"*50)\n",
        "\n",
        "else:\n",
        "    print(\"\\nâš ï¸ Test dosyasÄ± olmadÄ±ÄŸÄ± iÃ§in final testi atlandÄ±.\")\n",
        "\n",
        "# --- KAYDETME Ä°ÅLEMLERÄ° ---\n",
        "print(\">> Modeller 'model_stack' klasÃ¶rÃ¼ne kaydediliyor...\")\n",
        "joblib.dump(meta_model, 'model_stack/meta_model.pkl')\n",
        "joblib.dump(model_xgb, 'model_stack/xgb.pkl')\n",
        "joblib.dump(model_lgbm, 'model_stack/lgbm.pkl')\n",
        "joblib.dump(model_rf, 'model_stack/rf.pkl')\n",
        "joblib.dump(model_dt, 'model_stack/dt.pkl')\n",
        "joblib.dump(model_svm, 'model_stack/svm.pkl')\n",
        "joblib.dump(model_knn, 'model_stack/knn.pkl')\n",
        "joblib.dump(model_nb, 'model_stack/nb.pkl')\n",
        "joblib.dump(model_lr_base, 'model_stack/lr_base.pkl')\n",
        "\n",
        "with open('model_stack/dna_tok.pickle', 'wb') as f: pickle.dump(dna_tok, f)\n",
        "with open('model_stack/prot_tok.pickle', 'wb') as f: pickle.dump(prot_tok, f)\n",
        "\n",
        "print(\"âœ… HER ÅEY HAZIR! EÄÄ°TÄ°M TAMAMLANDI.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTeJdlWYzOHe",
        "outputId": "fe5a8f7e-9053-4240-d1e7-0050235a6850"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> SÄ°STEM BAÅLATILIYOR (GÃœNCELLENMÄ°Å EÄÄ°TÄ°M MODU)...\n",
            "\n",
            "--- EÄÄ°TÄ°M VERÄ°SÄ° YÃœKLENÄ°YOR ---\n",
            ">> 'veri.txt' dosyasÄ± okunuyor...\n",
            "âœ… 'veri.txt' baÅŸarÄ±yla yÃ¼klendi. Toplam kayÄ±t: 100000\n",
            "\n",
            "--- TEST VERÄ°SÄ° YÃœKLENÄ°YOR ---\n",
            ">> 'test.txt' dosyasÄ± okunuyor...\n",
            "âœ… 'test.txt' baÅŸarÄ±yla yÃ¼klendi. Toplam kayÄ±t: 10000\n",
            "\n",
            ">> Tokenizer eÄŸitiliyor...\n",
            "\n",
            ">> EÄŸitim verisi 'Base' ve 'Meta' eÄŸitim setlerine ayrÄ±lÄ±yor...\n",
            "\n",
            ">> [1/9] CNN (Deep Learning) EÄŸitiliyor...\n",
            "    - Epoch 0 bitti. Loss: 0.0319\n",
            "    - Epoch 10 bitti. Loss: 0.0082\n",
            "    - Epoch 20 bitti. Loss: 0.0050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> [2/9] XGBoost EÄŸitiliyor... (Scale Pos Weight Aktif)\n",
            ">> [3/9] LightGBM EÄŸitiliyor... (Balanced)\n",
            ">> [4/9] Random Forest EÄŸitiliyor... (Balanced)\n",
            ">> [5/9] Decision Tree EÄŸitiliyor... (Balanced)\n",
            ">> [6/9] SVM EÄŸitiliyor... (Balanced)\n",
            ">> [7/9] KNN EÄŸitiliyor... (Distance Weighting)\n",
            ">> [8/9] Naive Bayes EÄŸitiliyor...\n",
            ">> [9/9] Lojistik Regresyon (Base) EÄŸitiliyor... (Balanced)\n",
            "\n",
            ">> [META] Tahminler ToplanÄ±yor (Stacking)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Meta Model (Hakem) eÄŸitiliyor...\n",
            "\n",
            "==================================================\n",
            "ğŸ† FÄ°NAL TEST SONUÃ‡LARI (Test.txt Verisi Ãœzerinde)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Accuracy : %99.87\n",
            "ğŸš€ F1 Score : %99.89\n",
            "--------------------------------------------------\n",
            "DetaylÄ± Rapor:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Negatif       1.00      1.00      1.00      4290\n",
            "     Pozitif       1.00      1.00      1.00      5710\n",
            "\n",
            "    accuracy                           1.00     10000\n",
            "   macro avg       1.00      1.00      1.00     10000\n",
            "weighted avg       1.00      1.00      1.00     10000\n",
            "\n",
            "==================================================\n",
            ">> Modeller 'model_stack' klasÃ¶rÃ¼ne kaydediliyor...\n",
            "âœ… HER ÅEY HAZIR! EÄÄ°TÄ°M TAMAMLANDI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "etoJHw2DqO5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DOSYA ADI: canli_test_zorlu.py\n",
        "import numpy as np\n",
        "import pickle\n",
        "import joblib\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, confusion_matrix, classification_report\n",
        "\n",
        "# --- EKRAN TEMÄ°ZLÄ°ÄÄ° ---\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# Renkler\n",
        "YESIL = \"\\033[92m\"\n",
        "KIRMIZI = \"\\033[91m\"\n",
        "MAVI = \"\\033[94m\"\n",
        "SARI = \"\\033[93m\"\n",
        "CYAN = \"\\033[96m\"\n",
        "MOR = \"\\033[95m\"\n",
        "RESET = \"\\033[0m\"\n",
        "\n",
        "print(f\"{SARI}>> CANLI TEST BAÅLIYOR (DENGELÄ° MOD & EZBER KONTROLÃœ)...{RESET}\")\n",
        "\n",
        "# --- 1. MODELLERÄ° YÃœKLE ---\n",
        "try:\n",
        "    with open('model_stack/dna_tok.pickle', 'rb') as f: dna_tok = pickle.load(f)\n",
        "    with open('model_stack/prot_tok.pickle', 'rb') as f: prot_tok = pickle.load(f)\n",
        "    model_cnn = load_model('model_stack/model_cnn.h5')\n",
        "    model_xgb = joblib.load('model_stack/xgb.pkl')\n",
        "    model_lgbm = joblib.load('model_stack/lgbm.pkl')\n",
        "    model_rf = joblib.load('model_stack/rf.pkl')\n",
        "    model_dt = joblib.load('model_stack/dt.pkl')\n",
        "    model_svm = joblib.load('model_stack/svm.pkl')\n",
        "    model_knn = joblib.load('model_stack/knn.pkl')\n",
        "    model_nb = joblib.load('model_stack/nb.pkl')\n",
        "    model_lr_base = joblib.load('model_stack/lr_base.pkl')\n",
        "    meta_model = joblib.load('model_stack/meta_model.pkl')\n",
        "    print(f\"{YESIL}>> Modeller yÃ¼klendi.{RESET}\")\n",
        "except Exception as e:\n",
        "    print(f\"{KIRMIZI}âŒ HATA: Modeller yÃ¼klenemedi!{RESET}\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. YARDIMCI FONKSÄ°YONLAR ---\n",
        "def generate_sequence_with_pattern(length, vocab, pattern=None):\n",
        "    seq = list(np.random.choice(list(vocab), size=length))\n",
        "    if pattern:\n",
        "        start = np.random.randint(0, length - len(pattern) + 1)\n",
        "        for i, char in enumerate(pattern):\n",
        "            seq[start + i] = char\n",
        "    return \"\".join(seq)\n",
        "\n",
        "# --- 3. TEST VERÄ°SÄ° ÃœRETÄ°CÄ° (GÃœNCELLENDÄ°) ---\n",
        "def canli_veri_uret(zorla_etiket=None):\n",
        "    if zorla_etiket is None:\n",
        "        hedef = 1 if random.random() > 0.5 else 0\n",
        "    else:\n",
        "        hedef = zorla_etiket\n",
        "\n",
        "    if hedef == 1: # HASTA\n",
        "        risk = np.random.uniform(0.55, 0.99)\n",
        "        cons = np.random.uniform(6.0, 10.0)\n",
        "        maf = np.random.uniform(0.00, 0.15)\n",
        "        hydro = np.random.uniform(-5.0, -1.0)\n",
        "        polar = np.random.uniform(2.0, 5.0)\n",
        "        weight_base = np.random.uniform(50.0, 90.0)\n",
        "    else: # SAÄLAM\n",
        "        risk = np.random.uniform(0.00, 0.50)\n",
        "        cons = np.random.uniform(0.0, 5.0)\n",
        "        maf = np.random.uniform(0.20, 0.50)\n",
        "        hydro = np.random.uniform(-1.0, 5.0)\n",
        "        polar = np.random.uniform(-5.0, 1.0)\n",
        "        weight_base = np.random.uniform(10.0, 50.0)\n",
        "\n",
        "    score = (risk * 0.50) + (cons/10 * 0.30) + ((0.5 - maf) * 0.1) + (abs(hydro)/5 * 0.1)\n",
        "    score += np.random.uniform(-0.03, 0.03) # GÃ¼rÃ¼ltÃ¼\n",
        "\n",
        "    gercek_etiket = 1 if score > 0.60 else 0\n",
        "\n",
        "    # Motif Ekleme\n",
        "    if gercek_etiket == 1:\n",
        "        dna = generate_sequence_with_pattern(11, \"ACGT\", pattern=\"CGT\") if random.random() > 0.2 else generate_sequence_with_pattern(11, \"ACGT\")\n",
        "        prot = generate_sequence_with_pattern(11, \"ACDEFGHIKLMNPQRSTVWY\", pattern=\"WW\") if random.random() > 0.2 else generate_sequence_with_pattern(11, \"ACDEFGHIKLMNPQRSTVWY\")\n",
        "        weight = weight_base + 10\n",
        "    else:\n",
        "        dna = generate_sequence_with_pattern(11, \"ACGT\", pattern=\"AAA\") if random.random() > 0.8 else generate_sequence_with_pattern(11, \"ACGT\")\n",
        "        prot = generate_sequence_with_pattern(11, \"ACDEFGHIKLMNPQRSTVWY\")\n",
        "        weight = weight_base - 10\n",
        "\n",
        "    return dna, prot, [risk, maf, cons, hydro, polar, weight], gercek_etiket\n",
        "\n",
        "# --- 4. TAHMÄ°N MEKANÄ°ZMASI ---\n",
        "def tahmin_et(dna, prot, feats):\n",
        "    s_d = pad_sequences(dna_tok.texts_to_sequences([dna]), maxlen=11, padding='post')\n",
        "    s_p = pad_sequences(prot_tok.texts_to_sequences([prot]), maxlen=11, padding='post')\n",
        "    f_n = np.array([feats]).astype('float32')\n",
        "\n",
        "    p_cnn = model_cnn.predict([s_d, s_p, f_n], verbose=0).flatten()[0]\n",
        "    p_xgb = model_xgb.predict_proba(f_n)[:, 1][0]\n",
        "    p_lgbm = model_lgbm.predict_proba(f_n)[:, 1][0]\n",
        "    p_rf = model_rf.predict_proba(f_n)[:, 1][0]\n",
        "    p_dt = model_dt.predict_proba(f_n)[:, 1][0]\n",
        "    p_svm = model_svm.predict_proba(f_n)[:, 1][0]\n",
        "    p_knn = model_knn.predict_proba(f_n)[:, 1][0]\n",
        "    p_nb = model_nb.predict_proba(f_n)[:, 1][0]\n",
        "    p_lr = model_lr_base.predict_proba(f_n)[:, 1][0]\n",
        "\n",
        "    stack_input = np.array([[p_cnn, p_xgb, p_lgbm, p_rf, p_dt, p_svm, p_knn, p_nb, p_lr]])\n",
        "    final_pred = meta_model.predict(stack_input)[0]\n",
        "\n",
        "    return final_pred\n",
        "\n",
        "# --- 5. SÄ°MÃœLASYON ---\n",
        "y_true, y_pred = [], []\n",
        "TOPLAM_TEST = 1000\n",
        "YARI_TEST = TOPLAM_TEST // 2\n",
        "\n",
        "print(f\"\\n{'TÃœR':<10} | {'DNA':<12} | {'RÄ°SK':<5} | {'GERÃ‡EK':<8} | {'TAHMÄ°N':<8} | {'SONUÃ‡'}\")\n",
        "print(\"-\" * 75)\n",
        "\n",
        "for i in range(TOPLAM_TEST):\n",
        "    hedef = 1 if i < YARI_TEST else 0\n",
        "    dna, prot, feats, gercek = canli_veri_uret(zorla_etiket=hedef)\n",
        "    pred = tahmin_et(dna, prot, feats)\n",
        "\n",
        "    y_true.append(gercek)\n",
        "    y_pred.append(pred)\n",
        "\n",
        "    if i < 3 or (i > YARI_TEST-2 and i < YARI_TEST+2) or i > TOPLAM_TEST-3:\n",
        "        str_gercek = \"HASTA\" if gercek == 1 else \"SAÄLAM\"\n",
        "        str_tahmin = \"HASTA\" if pred == 1 else \"SAÄLAM\"\n",
        "        ikon = f\"{YESIL}âœ…{RESET}\" if gercek == pred else f\"{KIRMIZI}âŒ{RESET}\"\n",
        "        print(f\"{' [TEST] ': <10} | {dna[:8]:<12} | {feats[0]:.2f}  | {str_gercek:<8} | {str_tahmin:<8} | {ikon}\")\n",
        "\n",
        "# --- 6. RAPORLAMA VE EZBER ANALÄ°ZÄ° ---\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "kappa = cohen_kappa_score(y_true, y_pred)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"ğŸ“Š FÄ°NAL PERFORMANS VE EZBER RAPORU\")\n",
        "print(\"=\"*50)\n",
        "print(f\"âœ… DoÄŸruluk (Accuracy) : %{acc*100:.2f}\")\n",
        "print(f\"ğŸ§  Cohen's Kappa       : {kappa:.4f}\")\n",
        "\n",
        "print(\"\\nğŸ” EZBER ANALÄ°ZÄ° (MEMORIZATION CHECK):\")\n",
        "print(\"-\" * 50)\n",
        "if kappa > 0.80:\n",
        "    print(f\"{YESIL}ğŸŒŸ MÃœKEMMEL! Model EZBER YAPMAMIÅ.{RESET}\")\n",
        "    print(f\"   Model mantÄ±ÄŸÄ± kavramÄ±ÅŸ ve hiÃ§ gÃ¶rmediÄŸi verileri Ã§Ã¶zebiliyor.\")\n",
        "    print(f\"   (Kappa 0.80 Ã¼zeri 'Neredeyse Kusursuz' uyum demektir)\")\n",
        "elif kappa > 0.60:\n",
        "    print(f\"{SARI}âš ï¸ Ä°YÄ° AMA RÄ°SKLÄ°. Biraz ezber olabilir.{RESET}\")\n",
        "elif kappa < 0.40:\n",
        "    print(f\"{KIRMIZI}âŒ KÃ–TÃœ. Model muhtemelen ÅŸans eseri tutturuyor.{RESET}\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(f\"ğŸ§© KARMAÅIKLIK MATRÄ°SÄ°:\")\n",
        "print(f\"   GerÃ§ek SAÄLAM: {tn} | YanlÄ±ÅŸ Alarm: {fp}\")\n",
        "print(f\"   GerÃ§ek HASTA : {tp} | KaÃ§Ä±rÄ±lan   : {fn}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XLUhhZ05MnE",
        "outputId": "49a9e9fb-8858-48f8-cb09-90fb822e4454"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93m>> CANLI TEST BAÅLIYOR (DENGELÄ° MOD & EZBER KONTROLÃœ)...\u001b[0m\n",
            "\u001b[92m>> Modeller yÃ¼klendi.\u001b[0m\n",
            "\n",
            "TÃœR        | DNA          | RÄ°SK  | GERÃ‡EK   | TAHMÄ°N   | SONUÃ‡\n",
            "---------------------------------------------------------------------------\n",
            " [TEST]    | TGATTTCG     | 0.85  | HASTA    | HASTA    | \u001b[92mâœ…\u001b[0m\n",
            " [TEST]    | CCTCGTCT     | 0.91  | HASTA    | HASTA    | \u001b[92mâœ…\u001b[0m\n",
            " [TEST]    | GCGTCAAC     | 0.84  | HASTA    | HASTA    | \u001b[92mâœ…\u001b[0m\n",
            " [TEST]    | ACCCCGTT     | 0.96  | HASTA    | HASTA    | \u001b[92mâœ…\u001b[0m\n",
            " [TEST]    | TCAACACC     | 0.36  | SAÄLAM   | SAÄLAM   | \u001b[92mâœ…\u001b[0m\n",
            " [TEST]    | TGTTGGTG     | 0.25  | SAÄLAM   | SAÄLAM   | \u001b[92mâœ…\u001b[0m\n",
            " [TEST]    | CTATCTCT     | 0.04  | SAÄLAM   | SAÄLAM   | \u001b[92mâœ…\u001b[0m\n",
            " [TEST]    | TATCCCAA     | 0.05  | SAÄLAM   | SAÄLAM   | \u001b[92mâœ…\u001b[0m\n",
            "\n",
            "==================================================\n",
            "ğŸ“Š FÄ°NAL PERFORMANS VE EZBER RAPORU\n",
            "==================================================\n",
            "âœ… DoÄŸruluk (Accuracy) : %99.70\n",
            "ğŸ§  Cohen's Kappa       : 0.9940\n",
            "\n",
            "ğŸ” EZBER ANALÄ°ZÄ° (MEMORIZATION CHECK):\n",
            "--------------------------------------------------\n",
            "\u001b[92mğŸŒŸ MÃœKEMMEL! Model EZBER YAPMAMIÅ.\u001b[0m\n",
            "   Model mantÄ±ÄŸÄ± kavramÄ±ÅŸ ve hiÃ§ gÃ¶rmediÄŸi verileri Ã§Ã¶zebiliyor.\n",
            "   (Kappa 0.80 Ã¼zeri 'Neredeyse Kusursuz' uyum demektir)\n",
            "--------------------------------------------------\n",
            "ğŸ§© KARMAÅIKLIK MATRÄ°SÄ°:\n",
            "   GerÃ§ek SAÄLAM: 520 | YanlÄ±ÅŸ Alarm: 2\n",
            "   GerÃ§ek HASTA : 477 | KaÃ§Ä±rÄ±lan   : 1\n"
          ]
        }
      ]
    }
  ]
}
