{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtO/52FnlAjPuRa8GI/E03",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AstroBesat-SoftW/AstroBesat-SoftW/blob/main/Makine_Egitim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Concatenate, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "# --- 1. ADIM: VERİ ÜRETİMİ ---\n",
        "n_samples = 20000\n",
        "print(f\"TEKNOFEST 2026 Üniversite Şartnamesine uygun {n_samples} veri üretiliyor...\")\n",
        "\n",
        "def generate_bio_data(n):\n",
        "    sequences = []\n",
        "    bio_features = []\n",
        "    labels = []\n",
        "\n",
        "    for _ in range(n):\n",
        "        # 1. Sekans (11 harf)\n",
        "        seq = ''.join(np.random.choice(list('ACGT'), size=11))\n",
        "\n",
        "        # 2. Biyolojik Sayısal Veriler\n",
        "        risk_score = np.random.beta(2, 2)\n",
        "        maf = np.random.exponential(0.1)\n",
        "        if maf > 0.5: maf = 0.5\n",
        "        conservation = np.random.uniform(0, 10)\n",
        "        hydrophobicity = np.random.uniform(-5, 5)\n",
        "\n",
        "        # Etiketleme Mantığı\n",
        "        score_calc = (risk_score * 0.5) + (conservation/10 * 0.3) + ((0.5 - maf)*2 * 0.2)\n",
        "        score_calc += np.random.normal(0, 0.05)\n",
        "\n",
        "        if score_calc > 0.60:\n",
        "            labels.append(1) # Patojenik\n",
        "        else:\n",
        "            labels.append(0) # Benign\n",
        "\n",
        "        sequences.append(seq)\n",
        "        bio_features.append([risk_score, maf, conservation, hydrophobicity])\n",
        "\n",
        "    return sequences, np.array(bio_features), np.array(labels)\n",
        "\n",
        "sequences, X_numerical, y = generate_bio_data(n_samples)\n",
        "\n",
        "# --- 2. ADIM: EĞİTİM VERİSİNİ TEXT OLARAK KAYDETME ---\n",
        "print(\">> Veriler 'egitim_veri_seti.txt' dosyasına kaydediliyor...\")\n",
        "\n",
        "# Verileri bir DataFrame'de toplayıp dosyaya yazıyoruz\n",
        "df_ham_veri = pd.DataFrame(X_numerical, columns=['Risk_Skoru', 'MAF_Sikligi', 'Korunmusluk', 'Hidrofobiklik'])\n",
        "df_ham_veri.insert(0, 'DNA_Dizisi', sequences) # En başa diziyi ekle\n",
        "df_ham_veri['HEDEF_ETIKET'] = y # 1: Patojenik, 0: Benign\n",
        "\n",
        "# Dosyaya yazma (Tab ile ayrılmış TXT)\n",
        "df_ham_veri.to_csv('egitim_veri_seti.txt', sep='\\t', index=False)\n",
        "print(\">> Kayıt Başarılı! (Dosya adı: egitim_veri_seti.txt)\\n\")\n",
        "\n",
        "# --- 3. ADIM: ÖN İŞLEME VE MODEL HAZIRLIĞI ---\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(sequences)\n",
        "X_seq = tokenizer.texts_to_sequences(sequences)\n",
        "X_seq_pad = pad_sequences(X_seq, maxlen=11, padding='post')\n",
        "\n",
        "X_num = X_numerical # Zaten sayısal\n",
        "\n",
        "# Eğitim ve Test Ayrımı\n",
        "X_seq_train, X_seq_test, X_num_train, X_num_test, y_train, y_test = train_test_split(\n",
        "    X_seq_pad, X_num, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Model Mimarisi (Hybrid)\n",
        "input_seq = Input(shape=(11,), name=\"Sequence_Input\")\n",
        "x1 = Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=11)(input_seq)\n",
        "x1 = Conv1D(filters=16, kernel_size=3, activation='relu')(x1)\n",
        "x1 = GlobalMaxPooling1D()(x1)\n",
        "\n",
        "input_num = Input(shape=(4,), name=\"Bio_Features_Input\")\n",
        "x2 = Dense(16, activation='relu')(input_num)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "combined = Concatenate()([x1, x2])\n",
        "z = Dense(32, activation='relu')(combined)\n",
        "z = Dropout(0.3)(z)\n",
        "output = Dense(1, activation='sigmoid')(z)\n",
        "\n",
        "model = Model(inputs=[input_seq, input_num], outputs=output)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# --- 4. ADIM: EĞİTİM ---\n",
        "print(\"Model Eğitiliyor...\")\n",
        "model.fit([X_seq_train, X_num_train], y_train, epochs=8, batch_size=64, validation_split=0.1, verbose=1)\n",
        "\n",
        "# --- 5. ADIM: TEST VE SONUÇLARI KAYDETME ---\n",
        "print(\"\\n>> Test işlemi yapılıyor ve sonuçlar kaydediliyor...\")\n",
        "\n",
        "# Tahminleri al\n",
        "y_pred_prob = model.predict([X_seq_test, X_num_test])\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# F1 Skorunu Ekrana Bas\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"Test F1 Skoru: {f1:.4f}\")\n",
        "\n",
        "# --- KAYIT İŞLEMİ (TEST SONUÇLARI) ---\n",
        "# Test dizilerini (sayısal formattan) tekrar okunabilir harflere çevirelim\n",
        "test_seqs_text = tokenizer.sequences_to_texts(X_seq_test)\n",
        "# Tokenizer araya boşluk koyar (A C G T), onları siliyoruz (ACGT) ve büyük harf yapıyoruz\n",
        "test_seqs_text = [s.replace(' ', '').upper() for s in test_seqs_text]\n",
        "\n",
        "# Sonuç Tablosu Oluşturma\n",
        "df_sonuc = pd.DataFrame(X_num_test, columns=['Risk_Skoru', 'MAF_Sikligi', 'Korunmusluk', 'Hidrofobiklik'])\n",
        "df_sonuc.insert(0, 'Test_Edilen_DNA', test_seqs_text)\n",
        "df_sonuc['Gercek_Etiket'] = y_test\n",
        "df_sonuc['Tahmin_Olasiligi'] = y_pred_prob.flatten() # Virgüllü Olasılık\n",
        "df_sonuc['Tahmin_Sinifi'] = y_pred.flatten()         # 0 veya 1\n",
        "\n",
        "# Doğru bilip bilmediğini kontrol eden sütun\n",
        "df_sonuc['SONUC_DURUMU'] = np.where(df_sonuc['Gercek_Etiket'] == df_sonuc['Tahmin_Sinifi'], 'DOGRU_BILDI', 'HATA_YAPTI')\n",
        "\n",
        "# Dosyaya kaydetme\n",
        "df_sonuc.to_csv('test_sonuc_raporu.txt', sep='\\t', index=False)\n",
        "print(\">> Test sonuçları kaydedildi! (Dosya adı: test_sonuc_raporu.txt)\")\n",
        "print(\">> Dosyayı açıp hangi veride hata yaptığını 'SONUC_DURUMU' sütunundan inceleyebilirsiniz.\")"
      ],
      "metadata": {
        "id": "eTeJdlWYzOHe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}